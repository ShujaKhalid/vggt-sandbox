{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e174d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGT(\n",
       "  (aggregator): Aggregator(\n",
       "    (patch_embed): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (rope): RotaryPositionEmbedding2D()\n",
       "    (frame_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (global_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (camera_head): CameraHead(\n",
       "    (trunk): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (token_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (trunk_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_pose): Linear(in_features=9, out_features=2048, bias=True)\n",
       "    (poseLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    )\n",
       "    (adaln_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "    (pose_branch): Mlp(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (point_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (track_head): TrackHead(\n",
       "    (feature_extractor): DPTHead(\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (projects): ModuleList(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (resize_layers): ModuleList(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (2): Identity()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (refinenet1): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (output_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (tracker): BaseTrackerPredictor(\n",
       "      (corr_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=567, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (updateformer): EfficientUpdateFormer(\n",
       "        (input_norm): LayerNorm((388,), eps=1e-05, elementwise_affine=True)\n",
       "        (input_transform): Linear(in_features=388, out_features=384, bias=True)\n",
       "        (output_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (flow_head): Linear(in_features=384, out_features=130, bias=True)\n",
       "        (time_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_point2virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual2point_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fmap_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffeat_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (ffeat_updater): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (vis_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (conf_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT()\n",
    "_URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0cb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE=\"banana\"\n",
    "SKIP=1\n",
    "\n",
    "if SCENE==\"banana\": \n",
    "    # Load and preprocess example images (replace with your own image paths)\n",
    "    image_names = [\n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00001.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00002.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00003.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00004.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00005.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00006.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00007.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00008.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00009.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00010.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00011.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00012.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00013.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00014.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00015.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00016.JPG\"\n",
    "    ]\n",
    "    ### BANANA\n",
    "    width = 3008\n",
    "    height = 2000\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/banana\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"frame_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"lego\": \n",
    "    ### LEGO\n",
    "    image_names = [\"/home/skhalid/Documents/data/nerf_synthetic/lego/train/r_\"+str(v)+\".png\" for v in range(0, 99, SKIP)]\n",
    "    width = 800\n",
    "    height = 800\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/nerf_synthetic/lego/\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"r_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"bicycle\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/360_v2/bicycle/images_4/_DSC\"\n",
    "    image_names = [BASE+str(v)+\".JPG\" for v in range(8679, 8873, SKIP)]\n",
    "    width = 1236\n",
    "    height = 821    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/360_v2/bicycle\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"_DSC\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b124f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nextrinsic: torch.Size([1, 16, 3, 4])\\nintrinsic: torch.Size([1, 16, 3, 3])\\npoint_map_unproj: (16, 350, 518, 3)\\ndepth_map: torch.Size([1, 16, 350, 518, 1])\\ndepth_conf_map: torch.Size([1, 16, 350, 518])\\nbatch_tensor: torch.Size([1, 16, 3, 350, 518])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_batched_camera_inference(model, image_names, batch_size=8, device='cuda', dtype=torch.float16):\n",
    "    from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "    from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "    # from vggt.utils.io import load_and_preprocess_images\n",
    "\n",
    "    all_extrinsics = []\n",
    "    all_intrinsics = []\n",
    "    all_world_points = []\n",
    "    depth_maps = []\n",
    "    depth_conf_maps = []\n",
    "    batch_tensors = []\n",
    "\n",
    "    # Batch the rest of the images\n",
    "    print(f\"Processing the rest of {len(image_names)} images in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i + batch_size]\n",
    "        batch_tensor = load_and_preprocess_images(batch_names).to(device)\n",
    "\n",
    "        if i==0:\n",
    "            first_image = batch_tensor[0]\n",
    "            print(\"first_image.shape: {}\".format(batch_tensor.shape))\n",
    "        else:\n",
    "            # Add the first reference image to this batch as well\n",
    "            batch_tensor = torch.cat((first_image[None], batch_tensor), dim=0)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
    "            batch_tensor = batch_tensor[None]  # Add batch dim\n",
    "            agg_tokens, ps_idx = model.aggregator(batch_tensor)\n",
    "\n",
    "            pose_enc = model.camera_head(agg_tokens)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, batch_tensor.shape[-2:])\n",
    "\n",
    "            depth_map, depth_conf_map = model.depth_head(agg_tokens, batch_tensor, ps_idx)\n",
    "            \n",
    "            point_map_unproj = unproject_depth_map_to_point_map(depth_map.squeeze(0), extrinsic.squeeze(0), intrinsic.squeeze(0))\n",
    "    \n",
    "            # if i==0:    \n",
    "            #     print(\"batch: {} | point_map_unproj.shape: {}\".format(i, point_map_unproj.shape))\n",
    "            # else:\n",
    "            #     print(\"batch: {} | point_map_unproj[1:, ...].shape: {}\".format(i, point_map_unproj[1:, ...].shape))\n",
    "\n",
    "            if i==0:\n",
    "                all_extrinsics.append(extrinsic[0, ...])\n",
    "                all_intrinsics.append(intrinsic[0, ...])\n",
    "                all_world_points.append(point_map_unproj)\n",
    "                depth_maps.append(depth_map[0, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, ...])\n",
    "                batch_tensors.append(batch_tensor[0, ...])\n",
    "            else:\n",
    "                all_extrinsics.append(extrinsic[0, 1:])\n",
    "                all_intrinsics.append(intrinsic[0, 1:])\n",
    "                all_world_points.append(point_map_unproj[1:, ...])\n",
    "                depth_maps.append(depth_map[0, 1:, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, 1:, ...])\n",
    "                batch_tensors.append(batch_tensor[0, 1:, ...])\n",
    "\n",
    "            print(\"extrinsic: {}\".format(extrinsic.shape))\n",
    "            print(\"intrinsic: {}\".format(intrinsic.shape))\n",
    "            print(\"point_map_unproj: {}\".format(point_map_unproj.shape))\n",
    "            print(\"depth_map: {}\".format(depth_map.shape))\n",
    "            print(\"depth_conf_map: {}\".format(depth_conf_map.shape))\n",
    "            print(\"batch_tensor: {}\".format(batch_tensor.shape))\n",
    "\n",
    "    # Stack everything\n",
    "    batch_tensors = torch.cat(batch_tensors)  # [N, 4, 4]\n",
    "    all_extrinsics = torch.cat(all_extrinsics)  # [N, 4, 4]\n",
    "    all_intrinsics = torch.cat(all_intrinsics)  # [N, 3, 3]\n",
    "    all_world_points = np.concatenate(all_world_points)  # [N, H, W, 3]\n",
    "    depth_maps = torch.cat(depth_maps, dim=0)  # [N, H, W, 3]\n",
    "    depth_conf_maps = torch.cat(depth_conf_maps, dim=0)  # [N, H, W, 3]\n",
    "\n",
    "    return {\n",
    "        \"all_extrinsics\": all_extrinsics, \n",
    "        \"all_intrinsics\": all_intrinsics, \n",
    "        \"all_world_points\": all_world_points,\n",
    "        \"depth_maps\": depth_maps,\n",
    "        \"depth_conf_maps\": depth_conf_maps,\n",
    "        \"all_images\": batch_tensors\n",
    "    }\n",
    "\n",
    "'''\n",
    "extrinsic: torch.Size([1, 16, 3, 4])\n",
    "intrinsic: torch.Size([1, 16, 3, 3])\n",
    "point_map_unproj: (16, 350, 518, 3)\n",
    "depth_map: torch.Size([1, 16, 350, 518, 1])\n",
    "depth_conf_map: torch.Size([1, 16, 350, 518])\n",
    "batch_tensor: torch.Size([1, 16, 3, 350, 518])\n",
    "'''\n",
    "\n",
    "    # # Predict Tracks\n",
    "    # # choose your own points to track, with shape (N, 2) for one scene\n",
    "    # query_points = torch.FloatTensor([[100.0, 200.0], \n",
    "    #                                     [60.72, 259.94]]).to(device)\n",
    "    # track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00712d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     with torch.cuda.amp.autocast(dtype=dtype):\n",
    "#         # Predict attributes including cameras, depth maps, and point maps.\n",
    "#         # predictions = model(images)\n",
    "#         images = images[None]  # add batch dimension\n",
    "#         aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "                \n",
    "#     # Predict Cameras\n",
    "#     pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "#     # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)\n",
    "#     extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])\n",
    "\n",
    "#     # Predict Depth Maps\n",
    "#     depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)\n",
    "\n",
    "#     # Predict Point Maps\n",
    "#     point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)\n",
    "        \n",
    "#     # Construct 3D Points from Depth Maps and Cameras\n",
    "#     # which usually leads to more accurate 3D points than point map branch\n",
    "#     point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), \n",
    "#                                                                  extrinsic.squeeze(0), \n",
    "#                                                                  intrinsic.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4d6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=3\n",
    "\n",
    "# res = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "# all_extrinsics = res[\"all_extrinsics\"]\n",
    "# all_intrinsics = res[\"all_intrinsics\"]\n",
    "# all_world_points = res[\"all_world_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_world_points.shape)\n",
    "# print(all_extrinsics.shape)\n",
    "# print(all_intrinsics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38ddc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import json\n",
    "# from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# def pose_tensor_to_matrix(translation, quaternion):\n",
    "#     \"\"\"Convert translation + quaternion to 4x4 pose matrix.\"\"\"\n",
    "#     r = R.from_quat(quaternion.cpu().numpy())\n",
    "#     R_mat = r.as_matrix()  # [3, 3]\n",
    "\n",
    "#     pose_matrix = np.eye(4)\n",
    "#     pose_matrix[:3, :3] = R_mat\n",
    "#     pose_matrix[:3, 3] = translation.cpu().numpy()\n",
    "\n",
    "#     return pose_matrix.tolist()\n",
    "\n",
    "# def export_poses_as_json(pred_pose_tensor, output_path, image_folder=\"images\"):\n",
    "#     frames = []\n",
    "\n",
    "#     # If batched, flatten to a list\n",
    "#     poses = pred_pose_tensor.view(-1, pred_pose_tensor.shape[-1])  # [B * N, 10]\n",
    "\n",
    "#     for i, pose in enumerate(poses):\n",
    "#         translation = pose[0:3]\n",
    "#         quaternion = pose[3:7]\n",
    "#         fx = pose[7].item()\n",
    "#         fy = pose[8].item()\n",
    "#         cx = 0.5  # assuming normalized cx/cy; adjust as needed\n",
    "#         cy = 0.5\n",
    "\n",
    "#         transform_matrix = pose_tensor_to_matrix(translation, quaternion)\n",
    "\n",
    "#         frame = {\n",
    "#             \"file_path\": f\"{image_folder}/{i:06d}.png\",\n",
    "#             \"transform_matrix\": transform_matrix,\n",
    "#             \"intrinsics\": {\n",
    "#                 \"fx\": fx,\n",
    "#                 \"fy\": fy,\n",
    "#                 \"cx\": cx,\n",
    "#                 \"cy\": cy\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         print(frame)\n",
    "\n",
    "#     #     frames.append(frame)\n",
    "\n",
    "#     # data = {\n",
    "#     #     \"frames\": frames\n",
    "#     # }\n",
    "\n",
    "#     # with open(output_path, \"w\") as f:\n",
    "#     #     json.dump(data, f, indent=4)\n",
    "\n",
    "#     # print(f\"Saved poses to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc464ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_intrinsics_binary_from_poses(intrinsics_tensor, width, height, dims, output_path, camera_model=\"PINHOLE\", start_id=1):\n",
    "#     \"\"\"\n",
    "#     Write camera intrinsics from a tensor of 3x3 matrices.\n",
    "\n",
    "#     Args:\n",
    "#         intrinsics_tensor: Tensor of shape [B, N, 3, 3].\n",
    "#         width: Image width.\n",
    "#         height: Image height.\n",
    "#         output_path: Path to cameras.bin.\n",
    "#         camera_model: Camera model name.\n",
    "#         start_id: Starting camera ID.\n",
    "#     \"\"\"\n",
    "#     CAMERA_MODEL_IDS = {\n",
    "#         \"SIMPLE_PINHOLE\": 0,\n",
    "#         \"PINHOLE\": 1,\n",
    "#         \"SIMPLE_RADIAL\": 2,\n",
    "#         \"RADIAL\": 3,\n",
    "#         \"OPENCV\": 4,\n",
    "#     }\n",
    "#     model_id = CAMERA_MODEL_IDS.get(camera_model.upper(), 1)\n",
    "\n",
    "#     intrinsics_tensor = intrinsics_tensor.view(-1, 3, 3)  # [num_cameras, 3, 3]\n",
    "#     num_cameras = intrinsics_tensor.shape[0]\n",
    "#     w, h = dims\n",
    "\n",
    "\n",
    "#     SCALE = width / w # <---------------------------------------------------------------------------------------FIXME\n",
    "#     # SCALE = 2.0 # <---------------------------------------------------------------------------------------FIXME\n",
    "#     print(\"width: {} | w: {} | scale: {}\".format(width, w, SCALE))\n",
    "\n",
    "\n",
    "#     with open(output_path, \"wb\") as f:\n",
    "#         f.write(struct.pack(\"Q\", num_cameras))  # Number of cameras\n",
    "\n",
    "#         for i in range(num_cameras):\n",
    "#             intrinsics = intrinsics_tensor[i].cpu().numpy()\n",
    "#             fx = intrinsics[0, 0] * SCALE\n",
    "#             fy = intrinsics[1, 1] * SCALE\n",
    "#             cx = intrinsics[0, 2] * SCALE\n",
    "#             cy = intrinsics[1, 2] * SCALE\n",
    "\n",
    "#             params = [fx, fy, cx, cy]\n",
    "#             camera_id = start_id + i\n",
    "\n",
    "#             # Write: camera_id, model_id, width, height\n",
    "#             f.write(struct.pack(\"iiQQ\", camera_id, model_id, width, height))\n",
    "#             # Write parameters\n",
    "#             f.write(struct.pack(\"d\" * len(params), *params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f55f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_poses_as_json(predictions[\"pose_enc\"][0], \"output_path\", image_folder=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe8d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_colmap_format(fns, extrinsics_tensor, image_name_prefix=\"_DSC\", start_id=8679):\n",
    "#     \"\"\"\n",
    "#     Convert extrinsics matrices into COLMAP-format dictionaries.\n",
    "\n",
    "#     Args:\n",
    "#         extrinsics_tensor: Tensor of shape [B, N, 3, 4] containing [R|t].\n",
    "#         image_name_prefix: Prefix for image names.\n",
    "#         start_id: Starting ID for images.\n",
    "\n",
    "#     Returns:\n",
    "#         Dictionary of COLMAP-style extrinsics.\n",
    "#     \"\"\"\n",
    "#     images = {}\n",
    "#     extrinsics_tensor = extrinsics_tensor.view(-1, 3, 4)  # [num_poses, 3, 4]\n",
    "#     translation_scale_factor = 5.0  # Current (probably too small)\n",
    "#     print(extrinsics_tensor)\n",
    "\n",
    "#     for idx, extrinsic in enumerate(extrinsics_tensor, start=start_id):\n",
    "#         # Extract rotation matrix and translation\n",
    "#         R = extrinsic[:, :3].cpu().numpy()\n",
    "#         t = extrinsic[:, 3].cpu().numpy()\n",
    "\n",
    "#         # Convert rotation matrix to quaternion (COLMAP uses [w, x, y, z] format)\n",
    "#         from scipy.spatial.transform import Rotation\n",
    "#         qvec = Rotation.from_matrix(R).as_quat()  # [x, y, z, w]\n",
    "#         qvec_colmap = np.array([qvec[3], qvec[0], qvec[1], qvec[2]])  # Reorder to [w, x, y, z]\n",
    "#         qvec_colmap = qvec_colmap / np.linalg.norm(qvec_colmap)  # Ensure it's normalized\n",
    "\n",
    "#         image_name = f\"{image_name_prefix}{idx:05d}.JPG\"\n",
    "#         image_name = fns[idx]\n",
    "\n",
    "#         images[idx] = {\n",
    "#             \"id\": idx,\n",
    "#             \"qvec\": qvec_colmap,\n",
    "#             \"tvec\": t,\n",
    "#             \"camera_id\": 1,\n",
    "#             \"name\": image_name,\n",
    "#             \"xys\": np.zeros((0, 2)),\n",
    "#             \"point3D_ids\": np.array([])\n",
    "#         }\n",
    "\n",
    "#     return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53974676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import struct\n",
    "\n",
    "# def write_extrinsics_binary(images, output_path):\n",
    "#     with open(output_path, \"wb\") as f:\n",
    "#         # Write number of registered images (uint64)\n",
    "#         f.write(struct.pack(\"Q\", len(images)))\n",
    "\n",
    "#         for image_id, img in images.items():\n",
    "#             # Write: image_id (uint32), qvec (4 doubles), tvec (3 doubles), camera_id (uint32)\n",
    "#             f.write(struct.pack(\"i\", img[\"id\"]))  # IMAGE_ID\n",
    "#             f.write(struct.pack(\"dddd\", *img[\"qvec\"]))  # qw, qx, qy, qz\n",
    "#             f.write(struct.pack(\"ddd\", *img[\"tvec\"]))  # tx, ty, tz\n",
    "#             f.write(struct.pack(\"i\", img[\"camera_id\"]))  # CAMERA_ID\n",
    "\n",
    "#             # Write the image name (null-terminated string)\n",
    "#             f.write(img[\"name\"].encode(\"utf-8\") + b'\\x00')\n",
    "\n",
    "#             # No 2D points\n",
    "#             num_points2D = 0\n",
    "#             f.write(struct.pack(\"Q\", num_points2D))\n",
    "\n",
    "#             # (Skip point data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f854fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, h, w, c = all_world_points.shape\n",
    "# images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "# write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "# write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1549021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import open3d as o3d\n",
    "# from plyfile import PlyData, PlyElement\n",
    "\n",
    "\n",
    "# SINGLE_SAMPLE = False\n",
    "\n",
    "# if SINGLE_SAMPLE:\n",
    "#     all_images = load_and_preprocess_images(image_names).to(device)\n",
    "#     all_images = all_images[0][None, ...]\n",
    "#     # Remove batch dimension\n",
    "#     world_points = all_world_points[0]  # shape: [16, 350, 518, 3]\n",
    "# else:\n",
    "#     all_images = load_and_preprocess_images(image_names).to(device)\n",
    "#     # Remove batch dimension\n",
    "#     world_points = all_world_points  # shape: [16, 350, 518, 3]\n",
    "\n",
    "# print(all_images.shape)\n",
    "# print(all_world_points.shape)\n",
    "\n",
    "# # Flatten to [N, 3]\n",
    "# points_pmap = world_points.reshape(-1, 3)  # shape: [16 * 350 * 518, 3]\n",
    "# colors_pmap = all_images.permute(0,2,3,1).reshape(-1, 3).cpu().numpy()\n",
    "\n",
    "# # Convert to NumPy\n",
    "# points_np = points_pmap\n",
    "# colors_np = colors_pmap\n",
    "\n",
    "# # Optional: Remove invalid points (e.g., zero points)\n",
    "# mask = np.logical_and.reduce([np.isfinite(points_np[:, 0]),\n",
    "#                               np.isfinite(points_np[:, 1]),\n",
    "#                               np.isfinite(points_np[:, 2])])\n",
    "# points_np = points_np[mask]\n",
    "# colors_np = colors_np[mask]\n",
    "\n",
    "# if len(points_np) >= N:\n",
    "#     indices = np.random.choice(len(points_np), N, replace=False)\n",
    "#     sampled_points = points_np[indices]\n",
    "#     sampled_colors = colors_np[indices]\n",
    "# else:\n",
    "#     print(f\"Warning: Only {len(points_np)} points available, returning all.\")\n",
    "#     sampled_points = points_np\n",
    "#     sampled_colors = colors_np\n",
    "\n",
    "\n",
    "# def save_point_cloud_ply(filepath, points, colors=None, normals=None):\n",
    "#     \"\"\"\n",
    "#     Save a point cloud to a PLY file.\n",
    "    \n",
    "#     Args:\n",
    "#         filepath: Output path.\n",
    "#         points: (N, 3) numpy array of XYZ coordinates.\n",
    "#         colors: (N, 3) numpy array of RGB values in [0, 1] or [0, 255]. Optional.\n",
    "#         normals: (N, 3) numpy array of normals. Optional.\n",
    "#     \"\"\"\n",
    "\n",
    "#     num_points = points.shape[0]\n",
    "\n",
    "#     # Default colors → white\n",
    "#     if colors is None:\n",
    "#         colors = np.ones_like(points) * 255\n",
    "#     else:\n",
    "#         # If in [0, 1], scale to [0, 255]\n",
    "#         if colors.max() <= 1.0:\n",
    "#             colors = (colors * 255).astype(np.uint8)\n",
    "\n",
    "#     # Default normals → zero\n",
    "#     if normals is None:\n",
    "#         normals = np.zeros_like(points)\n",
    "\n",
    "#     # Define PLY vertex structure\n",
    "#     vertex_data = np.array(\n",
    "#         [\n",
    "#             (\n",
    "#                 points[i, 0], points[i, 1], points[i, 2],\n",
    "#                 normals[i, 0], normals[i, 1], normals[i, 2],\n",
    "#                 colors[i, 0], colors[i, 1], colors[i, 2],\n",
    "#             )\n",
    "#             for i in range(num_points)\n",
    "#         ],\n",
    "#         dtype=[\n",
    "#             ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "#             ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),\n",
    "#             ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Create PlyElement and write to file\n",
    "#     ply_element = PlyElement.describe(vertex_data, 'vertex')\n",
    "#     PlyData([ply_element]).write(filepath)\n",
    "\n",
    "#     print(f\"Saved point cloud with {num_points} points to {filepath}\")\n",
    "\n",
    "# normals = None\n",
    "# # Save as PLY\n",
    "# save_point_cloud_ply(PTS_PATH, sampled_points, sampled_colors, normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5178a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial import Delaunay\n",
    "from skimage.measure import marching_cubes\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "def visualize_delaunay_tetrahedra(points):\n",
    "    \"\"\"\n",
    "    Visualize the tetrahedral mesh from Delaunay triangulation.\n",
    "    Each tetrahedron is rendered as its wireframe using LineSet.\n",
    "    \"\"\"\n",
    "    delaunay = Delaunay(points)\n",
    "    tets = delaunay.simplices  # shape (M, 4), each row is a tetrahedron\n",
    "\n",
    "    lines = []\n",
    "    line_colors = []\n",
    "    for tet in tets:\n",
    "        for i, j in combinations(tet, 2):  # All 6 edges of the tetrahedron\n",
    "            lines.append([i, j])\n",
    "            line_colors.append([0, 0, 1])  # blue lines\n",
    "\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(line_colors)\n",
    "\n",
    "    # Optional: add point cloud for reference\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.paint_uniform_color([1, 0, 0])  # red\n",
    "\n",
    "    o3d.visualization.draw_geometries([line_set, pcd])\n",
    "\n",
    "def build_scalar_field_from_points(points, grid_resolution=128, padding=0.05):\n",
    "    \"\"\"\n",
    "    Convert a set of 3D points into a scalar field (SDF approximation) using nearest distances.\n",
    "    \"\"\"\n",
    "    min_bound = points.min(axis=0) - padding\n",
    "    max_bound = points.max(axis=0) + padding\n",
    "    grid_dims = (max_bound - min_bound)\n",
    "    voxel_size = grid_dims / grid_resolution\n",
    "\n",
    "    xs = np.linspace(min_bound[0], max_bound[0], grid_resolution)\n",
    "    ys = np.linspace(min_bound[1], max_bound[1], grid_resolution)\n",
    "    zs = np.linspace(min_bound[2], max_bound[2], grid_resolution)\n",
    "    grid = np.stack(np.meshgrid(xs, ys, zs, indexing=\"ij\"), -1).reshape(-1, 3)\n",
    "\n",
    "    # Compute distances to nearest point (Euclidean)\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=1).fit(points)\n",
    "    dists, _ = nn.kneighbors(grid)\n",
    "    sdf = dists.reshape((grid_resolution, grid_resolution, grid_resolution))\n",
    "\n",
    "    return sdf, min_bound, voxel_size[0]\n",
    "\n",
    "def marching_tetrahedra_from_points(points, vertex_colors, image=None):\n",
    "    \"\"\"\n",
    "    Given 3D points and an optional image, builds a mesh using marching tetrahedra.\n",
    "    \"\"\"\n",
    "    # Step 1: Delaunay tetrahedralization (not directly used for marching, but conceptually relevant)\n",
    "    delaunay = Delaunay(points)\n",
    "\n",
    "    print(delaunay.shape)\n",
    "\n",
    "    # Step 2: Build scalar field (SDF approximation)\n",
    "    sdf, origin, voxel_size = build_scalar_field_from_points(points)\n",
    "\n",
    "    # Step 3: Apply marching cubes (works like marching tetrahedra on voxels)\n",
    "    verts, faces, normals, _ = marching_cubes(sdf, level=0.2)\n",
    "\n",
    "    # Step 4: Map back to world coordinates\n",
    "    verts = verts * voxel_size + origin\n",
    "\n",
    "    # Step 5: Create Open3D mesh\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)  # shape (N, 3)\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_tetra_faces(points):\n",
    "    from itertools import combinations\n",
    "    delaunay = Delaunay(points)\n",
    "\n",
    "    faces = set()\n",
    "    for tet in delaunay.simplices:\n",
    "        for tri in combinations(tet, 3):\n",
    "            tri = tuple(sorted(tri))\n",
    "            if tri in faces:\n",
    "                faces.remove(tri)  # Internal face, shared twice\n",
    "            else:\n",
    "                faces.add(tri)     # Surface face\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(points)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(np.array(list(faces)))\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.5, 0.5, 0.5])\n",
    "\n",
    "    o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "\n",
    "def delaunay_to_pyg(points):\n",
    "    \"\"\"\n",
    "    Convert a point cloud to a PyTorch Geometric graph using 3D Delaunay triangulation.\n",
    "    \"\"\"\n",
    "    if isinstance(points, torch.Tensor):\n",
    "        points_np = points.cpu().numpy()\n",
    "    else:\n",
    "        points_np = points\n",
    "\n",
    "    # Step 1: Delaunay triangulation\n",
    "    delaunay = Delaunay(points_np)\n",
    "    simplices = delaunay.simplices  # shape (N_tets, 4)\n",
    "\n",
    "    # Step 2: Extract all tetrahedron edges (combinations of 2 from 4)\n",
    "    from itertools import combinations\n",
    "    edge_set = set()\n",
    "    for tet in simplices:\n",
    "        for i, j in combinations(tet, 2):\n",
    "            edge_set.add((i, j))\n",
    "            edge_set.add((j, i))  # For undirected graph\n",
    "\n",
    "    # Step 3: Create edge index tensor\n",
    "    edge_index = torch.tensor(list(edge_set), dtype=torch.long).t().contiguous()  # shape (2, E)\n",
    "\n",
    "    # Step 4: Create PyG graph\n",
    "    x = torch.tensor(points_np, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "    return data\n",
    "\n",
    "def sample_marching(pcd):\n",
    "    # Convert to a dense voxel grid (binary occupancy volume)\n",
    "    voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd, voxel_size=0.001)\n",
    "\n",
    "    # Convert voxels to a dense 3D grid for marching cubes\n",
    "    voxels = voxel_grid.get_voxels()\n",
    "    voxel_coords = np.array([v.grid_index for v in voxels])\n",
    "    volume_shape = voxel_coords.max(axis=0) + 1\n",
    "\n",
    "    occupancy = np.zeros(volume_shape, dtype=np.uint8)\n",
    "    for v in voxels:\n",
    "        x, y, z = v.grid_index\n",
    "        occupancy[x, y, z] = 1\n",
    "\n",
    "    # Marching cubes using skimage\n",
    "    from skimage import measure\n",
    "\n",
    "    verts, faces, normals, values = measure.marching_cubes(occupancy, level=0.5)\n",
    "\n",
    "    # Convert to Open3D mesh\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "    return mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a2d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH=\"/home/skhalid/Documents/gaussian-splatting/output/bicycle/point_cloud/iteration_7000/point_cloud.ply\"\n",
    "# pcd = o3d.io.read_point_cloud(PATH)\n",
    "# sampled_points = np.asarray(pcd.points)\n",
    "\n",
    "# # Load or generate sample 3D points\n",
    "# # mesh = marching_tetrahedra_from_points(sampled_points, sampled_colors)\n",
    "# # mesh = visualize_delaunay_tetrahedra(sampled_points)\n",
    "# mesh = visualize_tetra_faces(sampled_points)\n",
    "\n",
    "# # Visualize\n",
    "# o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4e427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['images']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "486f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['depth']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aeade52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = predictions['depth_conf']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f172f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=12\n",
    "\n",
    "# predictions = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "# all_extrinsics = predictions[\"all_extrinsics\"]\n",
    "# all_intrinsics = predictions[\"all_intrinsics\"]\n",
    "# all_world_points = predictions[\"all_world_points\"]\n",
    "# depth_conf_maps = predictions[\"depth_conf_maps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7511c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# This kinda gives you an unnclean surface\n",
    "# TODO: Clean this!\n",
    "# '''\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import open3d as o3d\n",
    "\n",
    "# # feature = \"depth\"\n",
    "# feature = \"all_world_points\"\n",
    "\n",
    "# # N = 1\n",
    "# N = all_world_points.shape[0]\n",
    "# points_all = []\n",
    "\n",
    "# print(all_world_points.shape)\n",
    "\n",
    "# features = predictions[feature]\n",
    "# print(features.shape)\n",
    "# # features = torch.cat((predictions[\"world_points\"][..., 0:2], predictions[\"depth\"]), axis=-1)\n",
    "\n",
    "# for i in range(N):\n",
    "#     print(i, features.shape)\n",
    "#     # Assume world_points is your tensor\n",
    "#     points = torch.Tensor(features[i])  # shape: [350, 518, 3]\n",
    "\n",
    "#     conf_mask = depth_conf_maps[0, i].cpu()\n",
    "#     conf_mask /= conf_mask.max()\n",
    "#     conf_mask[conf_mask<0.5] = 0.0\n",
    "#     conf_mask[conf_mask>0.0] = 1.0\n",
    "\n",
    "#     # Reshape into (N, 3)\n",
    "#     print(points.shape, conf_mask.shape)\n",
    "#     points = points.view(-1, 3)\n",
    "#     conf_mask = conf_mask.view(-1, 1)\n",
    "\n",
    "#     points *= conf_mask\n",
    "#     print(points.shape)\n",
    "\n",
    "#     # Optional: filter out invalid points (NaNs or extreme values)\n",
    "#     mask = ~torch.isnan(torch.tensor(points)).any(dim=-1)\n",
    "#     points = points[mask]\n",
    "#     points_all.append(points)\n",
    "\n",
    "# points_all = np.array(points_all)\n",
    "# points_all = np.concatenate((points_all), axis=0)\n",
    "\n",
    "# print(points_all.shape)\n",
    "\n",
    "# # Visualize with Open3D\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = o3d.utility.Vector3dVector(points_all)\n",
    "# o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d62cb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = depth_conf_maps[0, 0].cpu().numpy()\n",
    "# test /= test.max()\n",
    "# test[test<0.5] = 0.0\n",
    "# print(test.min(), test.max())\n",
    "# plt.figure()\n",
    "# plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efbe5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_points = np.asarray(pcd.points)\n",
    "\n",
    "# # Load or generate sample 3D points\n",
    "# mesh = sample_marching(pcd)\n",
    "# # mesh = marching_tetrahedra_from_points(sampled_points, None)\n",
    "# # mesh = visualize_delaunay_tetrahedra(sampled_points)\n",
    "# # mesh = visualize_tetra_faces(sampled_points)\n",
    "\n",
    "# # # Visualize\n",
    "# o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bf63765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model = YOLO(\"yolo11n-seg\")\n",
    "\n",
    "# # results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
    "# # results = model([\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"])\n",
    "# # results = model([\"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"])\n",
    "# results = model([\"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"])\n",
    "\n",
    "# for result in results:\n",
    "#     result.show()\n",
    "\n",
    "# # fig = plt.figure(figsize=(10,10))\n",
    "# # plt.imshow(results.render()[0])\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf205ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "# from PIL import Image\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# import clip\n",
    "# import urllib.request\n",
    "# from torchvision import transforms\n",
    "\n",
    "# # Load image\n",
    "# # image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image_rgb = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "# # Step 1: Load Segment Anything Model\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# model_type = \"vit_l\"\n",
    "\n",
    "# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "# sam.eval().cuda()\n",
    "\n",
    "# predictor = SamPredictor(sam)\n",
    "# predictor.set_image(image_rgb)\n",
    "\n",
    "# # Step 2: Predict masks with SAM (automatic mode)\n",
    "# from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "# mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "# masks = mask_generator.generate(image_rgb)\n",
    "\n",
    "# # Step 3: Load CLIP\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# # Step 4: Classify each mask region with CLIP\n",
    "# def classify_mask_with_clip(image_rgb, mask, clip_model, preprocess):\n",
    "#     # Crop the mask region\n",
    "#     # x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8)).tolist()\n",
    "#     x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8))\n",
    "#     cropped = image_rgb[y0:y1, x0:x1]\n",
    "\n",
    "#     # Apply mask\n",
    "#     masked_image = image_rgb.copy()\n",
    "#     masked_image[~mask.astype(bool)] = 0  # Black out background\n",
    "    \n",
    "#     if cropped.shape[0] < 10 or cropped.shape[1] < 10:\n",
    "#         return \"Unknown\", 0.0\n",
    "\n",
    "#     pil_crop = Image.fromarray(cropped)\n",
    "#     image_input = preprocess(pil_crop).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Define your candidate labels\n",
    "#     labels = [\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]\n",
    "#     # text_inputs = torch.cat([clip.tokenize(f\"a cropped snippet of an image extracted from the tanks and temples dataset that looks like {c}\") for c in labels]).to(device)\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"the cropped image extracted from the mipnerf360 dataset that looks like {c}\") for c in labels]).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         image_features = clip_model.encode_image(image_input)\n",
    "#         text_features = clip_model.encode_text(text_inputs)\n",
    "\n",
    "#         image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#         text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#         label_id = similarity.argmax().item()\n",
    "\n",
    "#         confidence = similarity[0, label_id].item()\n",
    "#         if confidence < 0.3:\n",
    "#             return \"Unknown\", confidence\n",
    "\n",
    "#         return labels[label_id], confidence\n",
    "\n",
    "# # Step 5: Visualize masks and CLIP labels\n",
    "# output_img = image_rgb.copy()\n",
    "# for mask_data in masks:\n",
    "#     mask = mask_data['segmentation']\n",
    "#     label, confidence = classify_mask_with_clip(image_rgb, mask, clip_model, preprocess)\n",
    "\n",
    "#     if label == \"Unknown\":\n",
    "#         continue\n",
    "# # \n",
    "#     # Draw mask\n",
    "#     color = np.random.randint(0, 255, size=3)\n",
    "#     output_img[mask] = 0.6 * output_img[mask] + 0.4 * color\n",
    "# # \n",
    "#     # Draw label\n",
    "#     y, x = np.argwhere(mask).mean(axis=0).astype(int)\n",
    "#     cv2.putText(output_img, f\"{label} {confidence:.2f}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 2)\n",
    "\n",
    "# # Show results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(output_img)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Segment Anything + CLIP classification\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b67fc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load OWL-ViT\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\").to(device)\n",
    "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "\n",
    "# # Load SAM\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# sam = sam_model_registry[\"vit_l\"](checkpoint=sam_checkpoint).to(device).eval()\n",
    "# predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57857658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load image\n",
    "# image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# image_np = np.array(image)\n",
    "\n",
    "# # Text prompt\n",
    "# # texts = [[\"car\", \"bicycle\", \"trees\", \"grass\", \"ground\", \"bench\", \"lego\", \"fruit\"]]  # You can modify this list\n",
    "# texts = [[\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]]\n",
    "\n",
    "# # Prepare input for OWL-ViT\n",
    "# inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Detect with OWL-ViT\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Get boxes and scores\n",
    "# target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "# results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.3)[0]\n",
    "\n",
    "# # Run SAM\n",
    "# predictor.set_image(image_np)\n",
    "\n",
    "# # Process each box from OWL-ViT\n",
    "# for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
    "#     box = box.cpu().numpy().astype(int)\n",
    "#     x0, y0, x1, y1 = box\n",
    "#     print(f\"{texts[0][label]}: {score:.2f} at box {box}\")\n",
    "\n",
    "#     # SAM expects box in XYXY format\n",
    "#     input_box = np.array([x0, y0, x1, y1])\n",
    "#     masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)\n",
    "\n",
    "#     # Overlay mask\n",
    "#     mask = masks[0]\n",
    "#     overlay = image_np.copy()\n",
    "#     overlay[mask] = (255, 0, 0)  # Red mask\n",
    "\n",
    "#     # Draw bounding box\n",
    "#     cv2.rectangle(overlay, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "#     cv2.putText(overlay, f\"{texts[0][label]} {score:.2f}\", (x0, y0 - 10),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # Show\n",
    "#     plt.imshow(overlay)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a370999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the rest of 16 images in batches of 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_image.shape: torch.Size([16, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3175356/2951244733.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 16, 3, 4])\n",
      "intrinsic: torch.Size([1, 16, 3, 3])\n",
      "point_map_unproj: (16, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 16, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 16, 350, 518])\n",
      "batch_tensor: torch.Size([1, 16, 3, 350, 518])\n",
      "torch.Size([16, 3, 4]) torch.Size([16, 3, 3]) (16, 350, 518, 3) torch.Size([16, 350, 518, 1]) torch.Size([16, 350, 518]) torch.Size([16, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=20\n",
    "\n",
    "predictions = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "all_extrinsics = predictions[\"all_extrinsics\"]\n",
    "all_intrinsics = predictions[\"all_intrinsics\"]\n",
    "all_world_points = predictions[\"all_world_points\"]\n",
    "depth_maps = predictions[\"depth_maps\"]\n",
    "depth_conf_maps = predictions[\"depth_conf_maps\"]\n",
    "all_images = predictions[\"all_images\"]\n",
    "\n",
    "print(all_extrinsics.shape, all_intrinsics.shape, all_world_points.shape, depth_maps.shape, depth_conf_maps.shape, all_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682acf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 350, 518, 1]) torch.Size([16, 3, 3]) torch.Size([16, 3, 4]) torch.Size([16, 350, 518, 3])\n",
      "✅ Mesh has 207692 vertices and 383857 triangles.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def tsdf_fusion_from_tensors(depth_maps, all_intrinsics, all_extrinsics, images, voxel_length=0.005, sdf_trunc=0.04):\n",
    "    \"\"\"\n",
    "    depth_maps: torch tensor of shape (N, H, W, 1)\n",
    "    all_intrinsics: torch tensor of shape (N, 3, 3)\n",
    "    all_extrinsics: torch tensor of shape (N, 3, 4)\n",
    "    \"\"\"\n",
    "    N, H, W, _ = depth_maps.shape\n",
    "    volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "        voxel_length=voxel_length,\n",
    "        sdf_trunc=sdf_trunc,\n",
    "        color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "    # volume = o3d.pipelines.integration.UniformTSDFVolume(\n",
    "    #     length=300.0,  # 3m bounding cube\n",
    "    #     resolution=512,\n",
    "    #     sdf_trunc=0.04,\n",
    "    #     color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "\n",
    "    for i in range(N):\n",
    "        depth_np = depth_maps[i, ..., 0].cpu().numpy().astype(np.float32)  # (H, W)\n",
    "        image = images[i, ...].cpu().numpy().astype(np.float32)\n",
    "        depth_np = np.ascontiguousarray(depth_np, dtype=np.float32) * 1000.0\n",
    "        image_np = np.ascontiguousarray(255 * image, dtype=np.uint8)\n",
    "        depth_image = o3d.geometry.Image(depth_np)\n",
    "        color_image = o3d.geometry.Image(image_np)\n",
    "\n",
    "        # Dummy color image (white)\n",
    "        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(color_image, depth_image, convert_rgb_to_intensity=False)\n",
    "\n",
    "        # Intrinsics\n",
    "        K = all_intrinsics[i].cpu().numpy()\n",
    "        intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "        intrinsic.set_intrinsics(width=W, height=H, fx=K[0, 0], fy=K[1, 1], cx=K[0, 2], cy=K[1, 2])\n",
    "\n",
    "        # # Extrinsics (convert to 4x4 matrix)\n",
    "        # extrinsic = np.eye(4)\n",
    "        # extrinsic[:3, :4] = all_extrinsics[i].cpu().numpy()\n",
    "        # Extrinsics: convert 3x4 to 4x4 and invert\n",
    "        extrinsic_3x4 = all_extrinsics[i].cpu().numpy()\n",
    "        extrinsic_4x4 = np.eye(4)\n",
    "        extrinsic_4x4[:3,:4] = extrinsic_3x4\n",
    "        extrinsic_4x4 = np.linalg.inv(extrinsic_4x4)\n",
    "\n",
    "        # pcd = volume.extract_point_cloud()\n",
    "        # o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        try:\n",
    "            volume.integrate(rgbd, intrinsic, extrinsic_4x4)\n",
    "        except Exception as e:\n",
    "            print(f\"Integration failed at frame {i}: {e}\")\n",
    "\n",
    "        # # Generate point cloud\n",
    "        # pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        #     rgbd, intrinsic, extrinsic_4x4)\n",
    "\n",
    "        # print(f\"[Frame {i}] #Points: {len(pcd.points)}\")\n",
    "        # o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.title(\"Color\")\n",
    "        # plt.imshow(np.asarray(rgbd.color))\n",
    "        # plt.axis(\"off\")\n",
    "\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.title(\"Depth\")\n",
    "        # plt.imshow(np.asarray(rgbd.depth), cmap='gray')\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "\n",
    "        # print(f\"Frame {i}:\")\n",
    "        # print(\"  Depth min/max:\", depth_np.min(), depth_np.max())\n",
    "        # print(\"  K:\\n\", K)\n",
    "        # print(\"  Extrinsic:\\n\", extrinsic_4x4)\n",
    "\n",
    "    # pcd = volume.extract_point_cloud()\n",
    "    # o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    mesh = volume.extract_triangle_mesh()\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    if len(mesh.vertices) == 0:\n",
    "        print(\"⚠️ Mesh is empty! TSDF fusion failed.\")\n",
    "    else:\n",
    "        print(f\"✅ Mesh has {len(mesh.vertices)} vertices and {len(mesh.triangles)} triangles.\")\n",
    "        o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_surface(mesh):\n",
    "    o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":    \n",
    "    # Remove batch dim\n",
    "    _depth = depth_maps  # shape: [17, 350, 518, 1]\n",
    "    _all_intrinsics = all_intrinsics\n",
    "    _all_extrinsics = all_extrinsics\n",
    "    _all_images = all_images.permute(0,2,3,1)\n",
    "\n",
    "    # print(_all_intrinsics)\n",
    "    print(_depth.shape, _all_intrinsics.shape, _all_extrinsics.shape, _all_images.shape)\n",
    "    \n",
    "    mesh = tsdf_fusion_from_tensors(_depth, _all_intrinsics, _all_extrinsics, _all_images)\n",
    "    visualize_surface(mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "719998c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 350, 518, 1])\n"
     ]
    }
   ],
   "source": [
    "print(depth_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e0903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
