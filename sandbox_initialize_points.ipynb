{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e174d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT()\n",
    "_URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE=\"bicycle\"\n",
    "SKIP=25\n",
    "\n",
    "if SCENE==\"banana\": \n",
    "    # Load and preprocess example images (replace with your own image paths)\n",
    "    image_names = [\n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00001.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00002.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00003.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00004.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00005.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00006.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00007.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00008.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00009.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00010.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00011.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00012.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00013.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00014.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00015.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00016.JPG\"\n",
    "    ]\n",
    "    ### BANANA\n",
    "    width = 3008\n",
    "    height = 2000\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/banana\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"frame_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"lego\": \n",
    "    ### LEGO\n",
    "    image_names = [\"/home/skhalid/Documents/data/nerf_synthetic/lego/train/r_\"+str(v)+\".png\" for v in range(0, 99, SKIP)]\n",
    "    width = 800\n",
    "    height = 800\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/nerf_synthetic/lego/\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"r_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"bicycle\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/360_v2/bicycle/images_4/_DSC\"\n",
    "    image_names = [BASE+str(v)+\".JPG\" for v in range(8679, 8873, SKIP)]\n",
    "    width = 1236\n",
    "    height = 821    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/360_v2/bicycle\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"_DSC\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n",
    "\n",
    "elif SCENE==\"truck\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/\"\n",
    "    image_names = [BASE+str(v).zfill(6)+\".jpg\" for v in range(1, 252, SKIP)]\n",
    "    width = 1957\n",
    "    height = 1091    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/tandt_db/tandt/truck\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b124f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_batched_camera_inference(model, image_names, batch_size=8, device='cuda', dtype=torch.float16):\n",
    "    from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "    from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "    # from vggt.utils.io import load_and_preprocess_images\n",
    "\n",
    "    all_extrinsics = []\n",
    "    all_intrinsics = []\n",
    "    all_world_points = []\n",
    "    depth_maps = []\n",
    "    depth_conf_maps = []\n",
    "    batch_tensors = []\n",
    "\n",
    "    # Batch the rest of the images\n",
    "    print(f\"Processing the rest of {len(image_names)} images in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i + batch_size]\n",
    "        batch_tensor = load_and_preprocess_images(batch_names).to(device)\n",
    "\n",
    "        if i==0:\n",
    "            first_image = batch_tensor[0]\n",
    "            print(\"first_image.shape: {}\".format(batch_tensor.shape))\n",
    "        else:\n",
    "            # Add the first reference image to this batch as well\n",
    "            batch_tensor = torch.cat((first_image[None], batch_tensor), dim=0)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
    "            batch_tensor = batch_tensor[None]  # Add batch dim\n",
    "            agg_tokens, ps_idx = model.aggregator(batch_tensor)\n",
    "\n",
    "            pose_enc = model.camera_head(agg_tokens)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, batch_tensor.shape[-2:])\n",
    "\n",
    "            depth_map, depth_conf_map = model.depth_head(agg_tokens, batch_tensor, ps_idx)\n",
    "            \n",
    "            point_map_unproj = unproject_depth_map_to_point_map(depth_map.squeeze(0), extrinsic.squeeze(0), intrinsic.squeeze(0))\n",
    "    \n",
    "            # if i==0:    \n",
    "            #     print(\"batch: {} | point_map_unproj.shape: {}\".format(i, point_map_unproj.shape))\n",
    "            # else:\n",
    "            #     print(\"batch: {} | point_map_unproj[1:, ...].shape: {}\".format(i, point_map_unproj[1:, ...].shape))\n",
    "\n",
    "            if i==0:\n",
    "                all_extrinsics.append(extrinsic[0, ...])\n",
    "                all_intrinsics.append(intrinsic[0, ...])\n",
    "                all_world_points.append(point_map_unproj)\n",
    "                depth_maps.append(depth_map[0, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, ...])\n",
    "                batch_tensors.append(batch_tensor[0, ...])\n",
    "            else:\n",
    "                all_extrinsics.append(extrinsic[0, 1:])\n",
    "                all_intrinsics.append(intrinsic[0, 1:])\n",
    "                all_world_points.append(point_map_unproj[1:, ...])\n",
    "                depth_maps.append(depth_map[0, 1:, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, 1:, ...])\n",
    "                batch_tensors.append(batch_tensor[0, 1:, ...])\n",
    "\n",
    "            print(\"extrinsic: {}\".format(extrinsic.shape))\n",
    "            print(\"intrinsic: {}\".format(intrinsic.shape))\n",
    "            print(\"point_map_unproj: {}\".format(point_map_unproj.shape))\n",
    "            print(\"depth_map: {}\".format(depth_map.shape))\n",
    "            print(\"depth_conf_map: {}\".format(depth_conf_map.shape))\n",
    "            print(\"batch_tensor: {}\".format(batch_tensor.shape))\n",
    "\n",
    "    # Stack everything\n",
    "    batch_tensors = torch.cat(batch_tensors)  # [N, 4, 4]\n",
    "    all_extrinsics = torch.cat(all_extrinsics)  # [N, 4, 4]\n",
    "    all_intrinsics = torch.cat(all_intrinsics)  # [N, 3, 3]\n",
    "    all_world_points = np.concatenate(all_world_points)  # [N, H, W, 3]\n",
    "    depth_maps = torch.cat(depth_maps, dim=0)  # [N, H, W, 3]\n",
    "    depth_conf_maps = torch.cat(depth_conf_maps, dim=0)  # [N, H, W, 3]\n",
    "\n",
    "    return {\n",
    "        \"all_extrinsics\": all_extrinsics, \n",
    "        \"all_intrinsics\": all_intrinsics, \n",
    "        \"all_world_points\": all_world_points,\n",
    "        \"depth_maps\": depth_maps,\n",
    "        \"depth_conf_maps\": depth_conf_maps,\n",
    "        \"all_images\": batch_tensors\n",
    "    }\n",
    "\n",
    "    # # Predict Tracks\n",
    "    # # choose your own points to track, with shape (N, 2) for one scene\n",
    "    # query_points = torch.FloatTensor([[100.0, 200.0], \n",
    "    #                                     [60.72, 259.94]]).to(device)\n",
    "    # track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=3\n",
    "\n",
    "# res = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "# all_extrinsics = res[\"all_extrinsics\"]\n",
    "# all_intrinsics = res[\"all_intrinsics\"]\n",
    "# all_world_points = res[\"all_world_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_world_points.shape)\n",
    "# print(all_extrinsics.shape)\n",
    "# print(all_intrinsics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ddc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def pose_tensor_to_matrix(translation, quaternion):\n",
    "    \"\"\"Convert translation + quaternion to 4x4 pose matrix.\"\"\"\n",
    "    r = R.from_quat(quaternion.cpu().numpy())\n",
    "    R_mat = r.as_matrix()  # [3, 3]\n",
    "\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = R_mat\n",
    "    pose_matrix[:3, 3] = translation.cpu().numpy()\n",
    "\n",
    "    return pose_matrix.tolist()\n",
    "\n",
    "def export_poses_as_json(pred_pose_tensor, output_path, image_folder=\"images\"):\n",
    "    frames = []\n",
    "\n",
    "    # If batched, flatten to a list\n",
    "    poses = pred_pose_tensor.view(-1, pred_pose_tensor.shape[-1])  # [B * N, 10]\n",
    "\n",
    "    for i, pose in enumerate(poses):\n",
    "        translation = pose[0:3]\n",
    "        quaternion = pose[3:7]\n",
    "        fx = pose[7].item()\n",
    "        fy = pose[8].item()\n",
    "        cx = 0.5  # assuming normalized cx/cy; adjust as needed\n",
    "        cy = 0.5\n",
    "\n",
    "        transform_matrix = pose_tensor_to_matrix(translation, quaternion)\n",
    "\n",
    "        frame = {\n",
    "            \"file_path\": f\"{image_folder}/{i:06d}.png\",\n",
    "            \"transform_matrix\": transform_matrix,\n",
    "            \"intrinsics\": {\n",
    "                \"fx\": fx,\n",
    "                \"fy\": fy,\n",
    "                \"cx\": cx,\n",
    "                \"cy\": cy\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(frame)\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    data = {\n",
    "        \"frames\": frames\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Saved poses to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc464ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_intrinsics_binary_from_poses(intrinsics_tensor, width, height, dims, output_path, camera_model=\"PINHOLE\", start_id=1):\n",
    "    \"\"\"\n",
    "    Write camera intrinsics from a tensor of 3x3 matrices.\n",
    "\n",
    "    Args:\n",
    "        intrinsics_tensor: Tensor of shape [B, N, 3, 3].\n",
    "        width: Image width.\n",
    "        height: Image height.\n",
    "        output_path: Path to cameras.bin.\n",
    "        camera_model: Camera model name.\n",
    "        start_id: Starting camera ID.\n",
    "    \"\"\"\n",
    "    CAMERA_MODEL_IDS = {\n",
    "        \"SIMPLE_PINHOLE\": 0,\n",
    "        \"PINHOLE\": 1,\n",
    "        \"SIMPLE_RADIAL\": 2,\n",
    "        \"RADIAL\": 3,\n",
    "        \"OPENCV\": 4,\n",
    "    }\n",
    "    model_id = CAMERA_MODEL_IDS.get(camera_model.upper(), 1)\n",
    "\n",
    "    intrinsics_tensor = intrinsics_tensor.view(-1, 3, 3)  # [num_cameras, 3, 3]\n",
    "    num_cameras = intrinsics_tensor.shape[0]\n",
    "    w, h = dims\n",
    "\n",
    "\n",
    "    SCALE = width / w # <---------------------------------------------------------------------------------------FIXME\n",
    "    # SCALE = 2.0 # <---------------------------------------------------------------------------------------FIXME\n",
    "    print(\"width: {} | w: {} | scale: {}\".format(width, w, SCALE))\n",
    "\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack(\"Q\", num_cameras))  # Number of cameras\n",
    "\n",
    "        for i in range(num_cameras):\n",
    "            intrinsics = intrinsics_tensor[i].cpu().numpy()\n",
    "            fx = intrinsics[0, 0] * SCALE\n",
    "            fy = intrinsics[1, 1] * SCALE\n",
    "            cx = intrinsics[0, 2] * SCALE\n",
    "            cy = intrinsics[1, 2] * SCALE\n",
    "\n",
    "            params = [fx, fy, cx, cy]\n",
    "            camera_id = start_id + i\n",
    "\n",
    "            # Write: camera_id, model_id, width, height\n",
    "            f.write(struct.pack(\"iiQQ\", camera_id, model_id, width, height))\n",
    "            # Write parameters\n",
    "            f.write(struct.pack(\"d\" * len(params), *params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f55f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_poses_as_json(predictions[\"pose_enc\"][0], \"output_path\", image_folder=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_colmap_format(fns, extrinsics_tensor, image_name_prefix=\"_DSC\", start_id=8679):\n",
    "    \"\"\"\n",
    "    Convert extrinsics matrices into COLMAP-format dictionaries.\n",
    "\n",
    "    Args:\n",
    "        extrinsics_tensor: Tensor of shape [B, N, 3, 4] containing [R|t].\n",
    "        image_name_prefix: Prefix for image names.\n",
    "        start_id: Starting ID for images.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of COLMAP-style extrinsics.\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    extrinsics_tensor = extrinsics_tensor.view(-1, 3, 4)  # [num_poses, 3, 4]\n",
    "    translation_scale_factor = 5.0  # Current (probably too small)\n",
    "    print(extrinsics_tensor)\n",
    "\n",
    "    for idx, extrinsic in enumerate(extrinsics_tensor, start=start_id):\n",
    "        # Extract rotation matrix and translation\n",
    "        R = extrinsic[:, :3].cpu().numpy()\n",
    "        t = extrinsic[:, 3].cpu().numpy()\n",
    "\n",
    "        # Convert rotation matrix to quaternion (COLMAP uses [w, x, y, z] format)\n",
    "        from scipy.spatial.transform import Rotation\n",
    "        qvec = Rotation.from_matrix(R).as_quat()  # [x, y, z, w]\n",
    "        qvec_colmap = np.array([qvec[3], qvec[0], qvec[1], qvec[2]])  # Reorder to [w, x, y, z]\n",
    "        qvec_colmap = qvec_colmap / np.linalg.norm(qvec_colmap)  # Ensure it's normalized\n",
    "\n",
    "        image_name = f\"{image_name_prefix}{idx:05d}.JPG\"\n",
    "        image_name = fns[idx]\n",
    "\n",
    "        images[idx] = {\n",
    "            \"id\": idx,\n",
    "            \"qvec\": qvec_colmap,\n",
    "            \"tvec\": t,\n",
    "            \"camera_id\": 1,\n",
    "            \"name\": image_name,\n",
    "            \"xys\": np.zeros((0, 2)),\n",
    "            \"point3D_ids\": np.array([])\n",
    "        }\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53974676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def write_extrinsics_binary(images, output_path):\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        # Write number of registered images (uint64)\n",
    "        f.write(struct.pack(\"Q\", len(images)))\n",
    "\n",
    "        for image_id, img in images.items():\n",
    "            # Write: image_id (uint32), qvec (4 doubles), tvec (3 doubles), camera_id (uint32)\n",
    "            f.write(struct.pack(\"i\", img[\"id\"]))  # IMAGE_ID\n",
    "            f.write(struct.pack(\"dddd\", *img[\"qvec\"]))  # qw, qx, qy, qz\n",
    "            f.write(struct.pack(\"ddd\", *img[\"tvec\"]))  # tx, ty, tz\n",
    "            f.write(struct.pack(\"i\", img[\"camera_id\"]))  # CAMERA_ID\n",
    "\n",
    "            # Write the image name (null-terminated string)\n",
    "            f.write(img[\"name\"].encode(\"utf-8\") + b'\\x00')\n",
    "\n",
    "            # No 2D points\n",
    "            num_points2D = 0\n",
    "            f.write(struct.pack(\"Q\", num_points2D))\n",
    "\n",
    "            # (Skip point data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f854fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, h, w, c = all_world_points.shape\n",
    "# images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "# write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "# write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1549021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def save_point_cloud_ply(filepath, points, colors=None, normals=None):\n",
    "    \"\"\"\n",
    "    Save a point cloud to a PLY file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Output path.\n",
    "        points: (N, 3) numpy array of XYZ coordinates.\n",
    "        colors: (N, 3) numpy array of RGB values in [0, 1] or [0, 255]. Optional.\n",
    "        normals: (N, 3) numpy array of normals. Optional.\n",
    "    \"\"\"\n",
    "\n",
    "    num_points = points.shape[0]\n",
    "\n",
    "    # Default colors → white\n",
    "    if colors is None:\n",
    "        colors = np.ones_like(points) * 255\n",
    "    else:\n",
    "        # If in [0, 1], scale to [0, 255]\n",
    "        if colors.max() <= 1.0:\n",
    "            colors = (colors * 255).astype(np.uint8)\n",
    "\n",
    "    # Default normals → zero\n",
    "    if normals is None:\n",
    "        normals = np.zeros_like(points)\n",
    "\n",
    "    # Define PLY vertex structure\n",
    "    vertex_data = np.array(\n",
    "        [\n",
    "            (\n",
    "                points[i, 0], points[i, 1], points[i, 2],\n",
    "                normals[i, 0], normals[i, 1], normals[i, 2],\n",
    "                colors[i, 0], colors[i, 1], colors[i, 2],\n",
    "            )\n",
    "            for i in range(num_points)\n",
    "        ],\n",
    "        dtype=[\n",
    "            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "            ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),\n",
    "            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create PlyElement and write to file\n",
    "    ply_element = PlyElement.describe(vertex_data, 'vertex')\n",
    "    PlyData([ply_element]).write(filepath)\n",
    "\n",
    "    print(f\"Saved point cloud with {num_points} points to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['images']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['depth']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeade52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = predictions['depth_conf']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f172f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run inference here\n",
    "'''\n",
    "\n",
    "BATCH_SIZE=12\n",
    "SINGLE_SAMPLE = False\n",
    "normals = None\n",
    "\n",
    "predictions = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "all_extrinsics = predictions[\"all_extrinsics\"]\n",
    "all_intrinsics = predictions[\"all_intrinsics\"]\n",
    "all_world_points = predictions[\"all_world_points\"]\n",
    "depth_conf_maps = predictions[\"depth_conf_maps\"]\n",
    "\n",
    "b, h, w, c = all_world_points.shape\n",
    "images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)\n",
    "\n",
    "if SINGLE_SAMPLE:\n",
    "    all_images = load_and_preprocess_images(image_names).to(device)\n",
    "    all_images = all_images[0][None, ...]\n",
    "    # Remove batch dimension\n",
    "    world_points = all_world_points[0]  # shape: [16, 350, 518, 3]\n",
    "else:\n",
    "    all_images = load_and_preprocess_images(image_names).to(device)\n",
    "    # Remove batch dimension\n",
    "    world_points = all_world_points  # shape: [16, 350, 518, 3]\n",
    "\n",
    "print(all_images.shape)\n",
    "print(all_world_points.shape)\n",
    "\n",
    "# Flatten to [N, 3]\n",
    "points_pmap = world_points.reshape(-1, 3)  # shape: [16 * 350 * 518, 3]\n",
    "colors_pmap = all_images.permute(0,2,3,1).reshape(-1, 3).cpu().numpy()\n",
    "\n",
    "# Convert to NumPy\n",
    "points_np = points_pmap\n",
    "colors_np = colors_pmap\n",
    "\n",
    "# Optional: Remove invalid points (e.g., zero points)\n",
    "mask = np.logical_and.reduce([np.isfinite(points_np[:, 0]),\n",
    "                              np.isfinite(points_np[:, 1]),\n",
    "                              np.isfinite(points_np[:, 2])])\n",
    "points_np = points_np[mask]\n",
    "colors_np = colors_np[mask]\n",
    "\n",
    "if len(points_np) >= N:\n",
    "    indices = np.random.choice(len(points_np), N, replace=False)\n",
    "    sampled_points = points_np[indices]\n",
    "    sampled_colors = colors_np[indices]\n",
    "else:\n",
    "    print(f\"Warning: Only {len(points_np)} points available, returning all.\")\n",
    "    sampled_points = points_np\n",
    "    sampled_colors = colors_np\n",
    "\n",
    "# Save as PLY\n",
    "save_point_cloud_ply(PTS_PATH, sampled_points, sampled_colors, normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SAMPLE OBJECT SEGMENTATION (BACKUP)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf63765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model = YOLO(\"yolo11n-seg\")\n",
    "\n",
    "# # results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
    "# # results = model([\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"])\n",
    "# # results = model([\"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"])\n",
    "# results = model([\"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"])\n",
    "\n",
    "# for result in results:\n",
    "#     result.show()\n",
    "\n",
    "# # fig = plt.figure(figsize=(10,10))\n",
    "# # plt.imshow(results.render()[0])\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf205ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "# from PIL import Image\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# import clip\n",
    "# import urllib.request\n",
    "# from torchvision import transforms\n",
    "\n",
    "# # Load image\n",
    "# # image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image_rgb = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "# # Step 1: Load Segment Anything Model\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# model_type = \"vit_l\"\n",
    "\n",
    "# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "# sam.eval().cuda()\n",
    "\n",
    "# predictor = SamPredictor(sam)\n",
    "# predictor.set_image(image_rgb)\n",
    "\n",
    "# # Step 2: Predict masks with SAM (automatic mode)\n",
    "# from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "# mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "# masks = mask_generator.generate(image_rgb)\n",
    "\n",
    "# # Step 3: Load CLIP\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# # Step 4: Classify each mask region with CLIP\n",
    "# def classify_mask_with_clip(image_rgb, mask, clip_model, preprocess):\n",
    "#     # Crop the mask region\n",
    "#     # x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8)).tolist()\n",
    "#     x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8))\n",
    "#     cropped = image_rgb[y0:y1, x0:x1]\n",
    "\n",
    "#     # Apply mask\n",
    "#     masked_image = image_rgb.copy()\n",
    "#     masked_image[~mask.astype(bool)] = 0  # Black out background\n",
    "    \n",
    "#     if cropped.shape[0] < 10 or cropped.shape[1] < 10:\n",
    "#         return \"Unknown\", 0.0\n",
    "\n",
    "#     pil_crop = Image.fromarray(cropped)\n",
    "#     image_input = preprocess(pil_crop).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Define your candidate labels\n",
    "#     labels = [\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]\n",
    "#     # text_inputs = torch.cat([clip.tokenize(f\"a cropped snippet of an image extracted from the tanks and temples dataset that looks like {c}\") for c in labels]).to(device)\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"the cropped image extracted from the mipnerf360 dataset that looks like {c}\") for c in labels]).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         image_features = clip_model.encode_image(image_input)\n",
    "#         text_features = clip_model.encode_text(text_inputs)\n",
    "\n",
    "#         image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#         text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#         label_id = similarity.argmax().item()\n",
    "\n",
    "#         confidence = similarity[0, label_id].item()\n",
    "#         if confidence < 0.3:\n",
    "#             return \"Unknown\", confidence\n",
    "\n",
    "#         return labels[label_id], confidence\n",
    "\n",
    "# # Step 5: Visualize masks and CLIP labels\n",
    "# output_img = image_rgb.copy()\n",
    "# for mask_data in masks:\n",
    "#     mask = mask_data['segmentation']\n",
    "#     label, confidence = classify_mask_with_clip(image_rgb, mask, clip_model, preprocess)\n",
    "\n",
    "#     if label == \"Unknown\":\n",
    "#         continue\n",
    "# # \n",
    "#     # Draw mask\n",
    "#     color = np.random.randint(0, 255, size=3)\n",
    "#     output_img[mask] = 0.6 * output_img[mask] + 0.4 * color\n",
    "# # \n",
    "#     # Draw label\n",
    "#     y, x = np.argwhere(mask).mean(axis=0).astype(int)\n",
    "#     cv2.putText(output_img, f\"{label} {confidence:.2f}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 2)\n",
    "\n",
    "# # Show results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(output_img)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Segment Anything + CLIP classification\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load OWL-ViT\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\").to(device)\n",
    "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "\n",
    "# # Load SAM\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# sam = sam_model_registry[\"vit_l\"](checkpoint=sam_checkpoint).to(device).eval()\n",
    "# predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57857658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load image\n",
    "# image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# image_np = np.array(image)\n",
    "\n",
    "# # Text prompt\n",
    "# # texts = [[\"car\", \"bicycle\", \"trees\", \"grass\", \"ground\", \"bench\", \"lego\", \"fruit\"]]  # You can modify this list\n",
    "# texts = [[\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]]\n",
    "\n",
    "# # Prepare input for OWL-ViT\n",
    "# inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Detect with OWL-ViT\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Get boxes and scores\n",
    "# target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "# results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.3)[0]\n",
    "\n",
    "# # Run SAM\n",
    "# predictor.set_image(image_np)\n",
    "\n",
    "# # Process each box from OWL-ViT\n",
    "# for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
    "#     box = box.cpu().numpy().astype(int)\n",
    "#     x0, y0, x1, y1 = box\n",
    "#     print(f\"{texts[0][label]}: {score:.2f} at box {box}\")\n",
    "\n",
    "#     # SAM expects box in XYXY format\n",
    "#     input_box = np.array([x0, y0, x1, y1])\n",
    "#     masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)\n",
    "\n",
    "#     # Overlay mask\n",
    "#     mask = masks[0]\n",
    "#     overlay = image_np.copy()\n",
    "#     overlay[mask] = (255, 0, 0)  # Red mask\n",
    "\n",
    "#     # Draw bounding box\n",
    "#     cv2.rectangle(overlay, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "#     cv2.putText(overlay, f\"{texts[0][label]} {score:.2f}\", (x0, y0 - 10),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # Show\n",
    "#     plt.imshow(overlay)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
