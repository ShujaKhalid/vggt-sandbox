{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e174d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGT(\n",
       "  (aggregator): Aggregator(\n",
       "    (patch_embed): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (rope): RotaryPositionEmbedding2D()\n",
       "    (frame_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (global_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (camera_head): CameraHead(\n",
       "    (trunk): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (token_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (trunk_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_pose): Linear(in_features=9, out_features=2048, bias=True)\n",
       "    (poseLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    )\n",
       "    (adaln_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "    (pose_branch): Mlp(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (point_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (track_head): TrackHead(\n",
       "    (feature_extractor): DPTHead(\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (projects): ModuleList(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (resize_layers): ModuleList(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (2): Identity()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (refinenet1): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (output_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (tracker): BaseTrackerPredictor(\n",
       "      (corr_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=567, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (updateformer): EfficientUpdateFormer(\n",
       "        (input_norm): LayerNorm((388,), eps=1e-05, elementwise_affine=True)\n",
       "        (input_transform): Linear(in_features=388, out_features=384, bias=True)\n",
       "        (output_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (flow_head): Linear(in_features=384, out_features=130, bias=True)\n",
       "        (time_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_point2virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual2point_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fmap_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffeat_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (ffeat_updater): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (vis_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (conf_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT()\n",
    "_URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0cb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE=\"banana\"\n",
    "SKIP=1\n",
    "\n",
    "if SCENE==\"banana\": \n",
    "    # Load and preprocess example images (replace with your own image paths)\n",
    "    image_names = [\n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00001.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00002.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00003.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00004.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00005.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00006.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00007.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00008.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00009.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00010.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00011.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00012.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00013.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00014.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00015.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00016.JPG\"\n",
    "    ]\n",
    "    ### BANANA\n",
    "    width = 3008\n",
    "    height = 2000\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/banana\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"frame_\"\n",
    "    START_ID = 0\n",
    "    N = 1_000\n",
    "\n",
    "elif SCENE==\"lego\": \n",
    "    ### LEGO\n",
    "    image_names = [\"/home/skhalid/Documents/data/nerf_synthetic/lego/train/r_\"+str(v)+\".png\" for v in range(0, 99, SKIP)]\n",
    "    width = 800\n",
    "    height = 800\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/nerf_synthetic/lego/\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"r_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"bicycle\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/360_v2/bicycle/images_4/_DSC\"\n",
    "    image_names = [BASE+str(v)+\".JPG\" for v in range(8679, 8873, SKIP)]\n",
    "    width = 1236\n",
    "    height = 821    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/360_v2/bicycle\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"_DSC\"\n",
    "    START_ID = 0\n",
    "    N = 3_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n",
    "\n",
    "elif SCENE==\"truck\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/\"\n",
    "    image_names = [BASE+str(v).zfill(6)+\".jpg\" for v in range(1, 252, SKIP)]\n",
    "    width = 1957\n",
    "    height = 1091    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/tandt_db/tandt/truck\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b124f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_batched_camera_inference(model, image_names, batch_size=8, device='cuda', dtype=torch.float16):\n",
    "    from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "    from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "    # from vggt.utils.io import load_and_preprocess_images\n",
    "\n",
    "    all_extrinsics = []\n",
    "    all_intrinsics = []\n",
    "    all_world_points = []\n",
    "    depth_maps = []\n",
    "    depth_conf_maps = []\n",
    "    batch_tensors = []\n",
    "\n",
    "    # Batch the rest of the images\n",
    "    print(f\"Processing the rest of {len(image_names)} images in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i + batch_size]\n",
    "        batch_tensor = load_and_preprocess_images(batch_names).to(device)\n",
    "\n",
    "        if i==0:\n",
    "            first_image = batch_tensor[0]\n",
    "            print(\"first_image.shape: {}\".format(batch_tensor.shape))\n",
    "        else:\n",
    "            # Add the first reference image to this batch as well\n",
    "            batch_tensor = torch.cat((first_image[None], batch_tensor), dim=0)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
    "            batch_tensor = batch_tensor[None]  # Add batch dim\n",
    "            agg_tokens, ps_idx = model.aggregator(batch_tensor)\n",
    "\n",
    "            pose_enc = model.camera_head(agg_tokens)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, batch_tensor.shape[-2:])\n",
    "\n",
    "            depth_map, depth_conf_map = model.depth_head(agg_tokens, batch_tensor, ps_idx)\n",
    "            \n",
    "            point_map_unproj = unproject_depth_map_to_point_map(depth_map.squeeze(0), extrinsic.squeeze(0), intrinsic.squeeze(0))\n",
    "    \n",
    "            # if i==0:    \n",
    "            #     print(\"batch: {} | point_map_unproj.shape: {}\".format(i, point_map_unproj.shape))\n",
    "            # else:\n",
    "            #     print(\"batch: {} | point_map_unproj[1:, ...].shape: {}\".format(i, point_map_unproj[1:, ...].shape))\n",
    "\n",
    "            if i==0:\n",
    "                all_extrinsics.append(extrinsic[0, ...])\n",
    "                all_intrinsics.append(intrinsic[0, ...])\n",
    "                all_world_points.append(point_map_unproj)\n",
    "                depth_maps.append(depth_map[0, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, ...])\n",
    "                batch_tensors.append(batch_tensor[0, ...])\n",
    "            else:\n",
    "                all_extrinsics.append(extrinsic[0, 1:])\n",
    "                all_intrinsics.append(intrinsic[0, 1:])\n",
    "                all_world_points.append(point_map_unproj[1:, ...])\n",
    "                depth_maps.append(depth_map[0, 1:, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, 1:, ...])\n",
    "                batch_tensors.append(batch_tensor[0, 1:, ...])\n",
    "\n",
    "            print(\"extrinsic: {}\".format(extrinsic.shape))\n",
    "            print(\"intrinsic: {}\".format(intrinsic.shape))\n",
    "            print(\"point_map_unproj: {}\".format(point_map_unproj.shape))\n",
    "            print(\"depth_map: {}\".format(depth_map.shape))\n",
    "            print(\"depth_conf_map: {}\".format(depth_conf_map.shape))\n",
    "            print(\"batch_tensor: {}\".format(batch_tensor.shape))\n",
    "\n",
    "    # Stack everything\n",
    "    batch_tensors = torch.cat(batch_tensors)  # [N, 4, 4]\n",
    "    all_extrinsics = torch.cat(all_extrinsics)  # [N, 4, 4]\n",
    "    all_intrinsics = torch.cat(all_intrinsics)  # [N, 3, 3]\n",
    "    all_world_points = np.concatenate(all_world_points)  # [N, H, W, 3]\n",
    "    depth_maps = torch.cat(depth_maps, dim=0)  # [N, H, W, 3]\n",
    "    depth_conf_maps = torch.cat(depth_conf_maps, dim=0)  # [N, H, W, 3]\n",
    "\n",
    "    return {\n",
    "        \"all_extrinsics\": all_extrinsics, \n",
    "        \"all_intrinsics\": all_intrinsics, \n",
    "        \"all_world_points\": all_world_points,\n",
    "        \"depth_maps\": depth_maps,\n",
    "        \"depth_conf_maps\": depth_conf_maps,\n",
    "        \"all_images\": batch_tensors\n",
    "    }\n",
    "\n",
    "    # # Predict Tracks\n",
    "    # # choose your own points to track, with shape (N, 2) for one scene\n",
    "    # query_points = torch.FloatTensor([[100.0, 200.0], \n",
    "    #                                     [60.72, 259.94]]).to(device)\n",
    "    # track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4d6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=3\n",
    "\n",
    "# res = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "# all_extrinsics = res[\"all_extrinsics\"]\n",
    "# all_intrinsics = res[\"all_intrinsics\"]\n",
    "# all_world_points = res[\"all_world_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_world_points.shape)\n",
    "# print(all_extrinsics.shape)\n",
    "# print(all_intrinsics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38ddc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def pose_tensor_to_matrix(translation, quaternion):\n",
    "    \"\"\"Convert translation + quaternion to 4x4 pose matrix.\"\"\"\n",
    "    r = R.from_quat(quaternion.cpu().numpy())\n",
    "    R_mat = r.as_matrix()  # [3, 3]\n",
    "\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = R_mat\n",
    "    pose_matrix[:3, 3] = translation.cpu().numpy()\n",
    "\n",
    "    return pose_matrix.tolist()\n",
    "\n",
    "def export_poses_as_json(pred_pose_tensor, output_path, image_folder=\"images\"):\n",
    "    frames = []\n",
    "\n",
    "    # If batched, flatten to a list\n",
    "    poses = pred_pose_tensor.view(-1, pred_pose_tensor.shape[-1])  # [B * N, 10]\n",
    "\n",
    "    for i, pose in enumerate(poses):\n",
    "        translation = pose[0:3]\n",
    "        quaternion = pose[3:7]\n",
    "        fx = pose[7].item()\n",
    "        fy = pose[8].item()\n",
    "        cx = 0.5  # assuming normalized cx/cy; adjust as needed\n",
    "        cy = 0.5\n",
    "\n",
    "        transform_matrix = pose_tensor_to_matrix(translation, quaternion)\n",
    "\n",
    "        frame = {\n",
    "            \"file_path\": f\"{image_folder}/{i:06d}.png\",\n",
    "            \"transform_matrix\": transform_matrix,\n",
    "            \"intrinsics\": {\n",
    "                \"fx\": fx,\n",
    "                \"fy\": fy,\n",
    "                \"cx\": cx,\n",
    "                \"cy\": cy\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(frame)\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    data = {\n",
    "        \"frames\": frames\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Saved poses to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc464ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_intrinsics_binary_from_poses(intrinsics_tensor, width, height, dims, output_path, camera_model=\"PINHOLE\", start_id=1):\n",
    "    \"\"\"\n",
    "    Write camera intrinsics from a tensor of 3x3 matrices.\n",
    "\n",
    "    Args:\n",
    "        intrinsics_tensor: Tensor of shape [B, N, 3, 3].\n",
    "        width: Image width.\n",
    "        height: Image height.\n",
    "        output_path: Path to cameras.bin.\n",
    "        camera_model: Camera model name.\n",
    "        start_id: Starting camera ID.\n",
    "    \"\"\"\n",
    "    CAMERA_MODEL_IDS = {\n",
    "        \"SIMPLE_PINHOLE\": 0,\n",
    "        \"PINHOLE\": 1,\n",
    "        \"SIMPLE_RADIAL\": 2,\n",
    "        \"RADIAL\": 3,\n",
    "        \"OPENCV\": 4,\n",
    "    }\n",
    "    model_id = CAMERA_MODEL_IDS.get(camera_model.upper(), 1)\n",
    "\n",
    "    intrinsics_tensor = intrinsics_tensor.view(-1, 3, 3)  # [num_cameras, 3, 3]\n",
    "    num_cameras = intrinsics_tensor.shape[0]\n",
    "    w, h = dims\n",
    "\n",
    "\n",
    "    SCALE = width / w # <---------------------------------------------------------------------------------------FIXME\n",
    "    # SCALE = 2.0 # <---------------------------------------------------------------------------------------FIXME\n",
    "    print(\"width: {} | w: {} | scale: {}\".format(width, w, SCALE))\n",
    "\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack(\"Q\", num_cameras))  # Number of cameras\n",
    "\n",
    "        for i in range(num_cameras):\n",
    "            intrinsics = intrinsics_tensor[i].cpu().numpy()\n",
    "            fx = intrinsics[0, 0] * SCALE\n",
    "            fy = intrinsics[1, 1] * SCALE\n",
    "            cx = intrinsics[0, 2] * SCALE\n",
    "            cy = intrinsics[1, 2] * SCALE\n",
    "\n",
    "            params = [fx, fy, cx, cy]\n",
    "            camera_id = start_id + i\n",
    "\n",
    "            # Write: camera_id, model_id, width, height\n",
    "            f.write(struct.pack(\"iiQQ\", camera_id, model_id, width, height))\n",
    "            # Write parameters\n",
    "            f.write(struct.pack(\"d\" * len(params), *params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f55f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_poses_as_json(predictions[\"pose_enc\"][0], \"output_path\", image_folder=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe8d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_colmap_format(fns, extrinsics_tensor, image_name_prefix=\"_DSC\", start_id=8679):\n",
    "    \"\"\"\n",
    "    Convert extrinsics matrices into COLMAP-format dictionaries.\n",
    "\n",
    "    Args:\n",
    "        extrinsics_tensor: Tensor of shape [B, N, 3, 4] containing [R|t].\n",
    "        image_name_prefix: Prefix for image names.\n",
    "        start_id: Starting ID for images.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of COLMAP-style extrinsics.\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    extrinsics_tensor = extrinsics_tensor.view(-1, 3, 4)  # [num_poses, 3, 4]\n",
    "    translation_scale_factor = 5.0  # Current (probably too small)\n",
    "    print(extrinsics_tensor)\n",
    "\n",
    "    for idx, extrinsic in enumerate(extrinsics_tensor, start=start_id):\n",
    "        # Extract rotation matrix and translation\n",
    "        R = extrinsic[:, :3].cpu().numpy()\n",
    "        t = extrinsic[:, 3].cpu().numpy()\n",
    "\n",
    "        # Convert rotation matrix to quaternion (COLMAP uses [w, x, y, z] format)\n",
    "        from scipy.spatial.transform import Rotation\n",
    "        qvec = Rotation.from_matrix(R).as_quat()  # [x, y, z, w]\n",
    "        qvec_colmap = np.array([qvec[3], qvec[0], qvec[1], qvec[2]])  # Reorder to [w, x, y, z]\n",
    "        qvec_colmap = qvec_colmap / np.linalg.norm(qvec_colmap)  # Ensure it's normalized\n",
    "\n",
    "        image_name = f\"{image_name_prefix}{idx:05d}.JPG\"\n",
    "        image_name = fns[idx]\n",
    "\n",
    "        images[idx] = {\n",
    "            \"id\": idx,\n",
    "            \"qvec\": qvec_colmap,\n",
    "            \"tvec\": t,\n",
    "            \"camera_id\": 1,\n",
    "            \"name\": image_name,\n",
    "            \"xys\": np.zeros((0, 2)),\n",
    "            \"point3D_ids\": np.array([])\n",
    "        }\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53974676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def write_extrinsics_binary(images, output_path):\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        # Write number of registered images (uint64)\n",
    "        f.write(struct.pack(\"Q\", len(images)))\n",
    "\n",
    "        for image_id, img in images.items():\n",
    "            # Write: image_id (uint32), qvec (4 doubles), tvec (3 doubles), camera_id (uint32)\n",
    "            f.write(struct.pack(\"i\", img[\"id\"]))  # IMAGE_ID\n",
    "            f.write(struct.pack(\"dddd\", *img[\"qvec\"]))  # qw, qx, qy, qz\n",
    "            f.write(struct.pack(\"ddd\", *img[\"tvec\"]))  # tx, ty, tz\n",
    "            f.write(struct.pack(\"i\", img[\"camera_id\"]))  # CAMERA_ID\n",
    "\n",
    "            # Write the image name (null-terminated string)\n",
    "            f.write(img[\"name\"].encode(\"utf-8\") + b'\\x00')\n",
    "\n",
    "            # No 2D points\n",
    "            num_points2D = 0\n",
    "            f.write(struct.pack(\"Q\", num_points2D))\n",
    "\n",
    "            # (Skip point data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f854fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, h, w, c = all_world_points.shape\n",
    "# images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "# write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "# write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1549021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def save_point_cloud_ply(filepath, points, colors=None, normals=None):\n",
    "    \"\"\"\n",
    "    Save a point cloud to a PLY file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Output path.\n",
    "        points: (N, 3) numpy array of XYZ coordinates.\n",
    "        colors: (N, 3) numpy array of RGB values in [0, 1] or [0, 255]. Optional.\n",
    "        normals: (N, 3) numpy array of normals. Optional.\n",
    "    \"\"\"\n",
    "\n",
    "    num_points = points.shape[0]\n",
    "\n",
    "    # Default colors  white\n",
    "    if colors is None:\n",
    "        colors = np.ones_like(points) * 255\n",
    "    else:\n",
    "        # If in [0, 1], scale to [0, 255]\n",
    "        if colors.max() <= 1.0:\n",
    "            colors = (colors * 255).astype(np.uint8)\n",
    "\n",
    "    # Default normals  zero\n",
    "    if normals is None:\n",
    "        normals = np.zeros_like(points)\n",
    "\n",
    "    # Define PLY vertex structure\n",
    "    vertex_data = np.array(\n",
    "        [\n",
    "            (\n",
    "                points[i, 0], points[i, 1], points[i, 2],\n",
    "                normals[i, 0], normals[i, 1], normals[i, 2],\n",
    "                colors[i, 0], colors[i, 1], colors[i, 2],\n",
    "            )\n",
    "            for i in range(num_points)\n",
    "        ],\n",
    "        dtype=[\n",
    "            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "            ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),\n",
    "            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create PlyElement and write to file\n",
    "    ply_element = PlyElement.describe(vertex_data, 'vertex')\n",
    "    PlyData([ply_element]).write(filepath)\n",
    "\n",
    "    print(f\"Saved point cloud with {num_points} points to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee4e427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['images']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['depth']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aeade52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = predictions['depth_conf']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f172f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the rest of 16 images in batches of 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_image.shape: torch.Size([16, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4130446/2588084714.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|| 1/1 [00:01<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 16, 3, 4])\n",
      "intrinsic: torch.Size([1, 16, 3, 3])\n",
      "point_map_unproj: (16, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 16, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 16, 350, 518])\n",
      "batch_tensor: torch.Size([1, 16, 3, 350, 518])\n",
      "tensor([[[ 1.0000e+00, -5.4717e-05,  2.2769e-05, -1.4901e-06],\n",
      "         [ 5.4717e-05,  1.0000e+00, -8.1897e-05,  3.2306e-05],\n",
      "         [-2.2769e-05,  8.1897e-05,  1.0000e+00, -1.0312e-04]],\n",
      "\n",
      "        [[ 9.9529e-01,  3.3007e-02, -9.1150e-02,  1.2115e-01],\n",
      "         [-3.3312e-02,  9.9944e-01, -1.7226e-03, -3.7689e-03],\n",
      "         [ 9.1028e-02,  4.7474e-03,  9.9584e-01, -3.6488e-03]],\n",
      "\n",
      "        [[ 9.9724e-01,  1.0303e-02, -7.3572e-02,  1.0443e-01],\n",
      "         [-1.2547e-02,  9.9947e-01, -3.0131e-02,  1.3420e-02],\n",
      "         [ 7.3206e-02,  3.0986e-02,  9.9684e-01, -2.3621e-01]],\n",
      "\n",
      "        [[ 8.5601e-01, -2.4255e-01,  4.5653e-01, -4.5044e-01],\n",
      "         [ 3.0166e-01,  9.5154e-01, -6.0089e-02,  5.5786e-02],\n",
      "         [-4.1989e-01,  1.8918e-01,  8.8764e-01, -7.0007e-02]],\n",
      "\n",
      "        [[ 6.4812e-01, -3.7262e-01,  6.6423e-01, -6.8506e-01],\n",
      "         [ 4.3851e-01,  8.9556e-01,  7.4488e-02, -9.0088e-02],\n",
      "         [-6.2226e-01,  2.4305e-01,  7.4378e-01,  8.4839e-02]],\n",
      "\n",
      "        [[ 2.6874e-01, -4.5222e-01,  8.5078e-01, -8.1152e-01],\n",
      "         [ 6.6199e-01,  7.2803e-01,  1.7794e-01, -1.7871e-01],\n",
      "         [-6.9955e-01,  5.1515e-01,  4.9510e-01,  3.8135e-01]],\n",
      "\n",
      "        [[-1.7718e-01, -4.5473e-01,  8.7289e-01, -8.5596e-01],\n",
      "         [ 4.9325e-01,  7.2619e-01,  4.7887e-01, -4.6289e-01],\n",
      "         [-8.5143e-01,  5.1544e-01,  9.5416e-02,  7.0996e-01]],\n",
      "\n",
      "        [[-7.4967e-01, -3.5198e-01,  5.6093e-01, -5.5371e-01],\n",
      "         [ 3.7737e-01,  4.6934e-01,  7.9819e-01, -7.9248e-01],\n",
      "         [-5.4433e-01,  8.0991e-01, -2.1852e-01,  9.7217e-01]],\n",
      "\n",
      "        [[-6.6501e-01, -3.2072e-01,  6.7458e-01, -6.8555e-01],\n",
      "         [ 6.3900e-01,  2.2355e-01,  7.3600e-01, -6.9238e-01],\n",
      "         [-3.8701e-01,  9.2024e-01,  5.6365e-02,  8.3301e-01]],\n",
      "\n",
      "        [[-9.1327e-01, -4.0713e-01,  1.5115e-02, -6.4575e-02],\n",
      "         [-1.2872e-01,  3.2324e-01,  9.3713e-01, -9.4092e-01],\n",
      "         [-3.8641e-01,  8.5424e-01, -3.4768e-01,  1.0430e+00]],\n",
      "\n",
      "        [[-9.6626e-01,  1.3199e-01, -2.2124e-01,  1.5002e-01],\n",
      "         [-2.1648e-01,  4.9594e-02,  9.7531e-01, -9.5752e-01],\n",
      "         [ 1.3968e-01,  9.8997e-01, -1.9758e-02,  1.0254e+00]],\n",
      "\n",
      "        [[-2.7357e-01,  4.2656e-01, -8.6191e-01,  8.6084e-01],\n",
      "         [-3.6112e-01,  7.8489e-01,  5.0347e-01, -4.9707e-01],\n",
      "         [ 8.9121e-01,  4.4927e-01, -6.0658e-02,  8.2471e-01]],\n",
      "\n",
      "        [[ 3.0009e-01,  3.5118e-01, -8.8672e-01,  8.9502e-01],\n",
      "         [-6.4724e-01,  7.5783e-01,  8.1332e-02, -1.2683e-01],\n",
      "         [ 7.0040e-01,  5.4969e-01,  4.5470e-01,  4.2944e-01]],\n",
      "\n",
      "        [[ 6.7920e-01, -1.1743e-01, -7.2461e-01,  7.1338e-01],\n",
      "         [-5.0488e-01,  6.4160e-01, -5.7715e-01,  5.1855e-01],\n",
      "         [ 5.3320e-01,  7.5781e-01,  3.7646e-01,  5.4443e-01]],\n",
      "\n",
      "        [[-1.7914e-01,  7.2634e-01, -6.6284e-01,  5.6250e-01],\n",
      "         [-9.3345e-01,  8.7556e-02,  3.4754e-01, -3.6060e-01],\n",
      "         [ 3.1066e-01,  6.8091e-01,  6.6272e-01,  2.0349e-01]],\n",
      "\n",
      "        [[ 9.6228e-01,  1.9298e-01, -1.9176e-01,  2.4060e-01],\n",
      "         [-2.0812e-01,  9.7608e-01, -6.2558e-02,  2.0477e-02],\n",
      "         [ 1.7516e-01,  1.0015e-01,  9.7943e-01, -9.7900e-02]]],\n",
      "       device='cuda:0')\n",
      "width: 3008 | w: 518 | scale: 5.806949806949807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 350, 518])\n",
      "(16, 350, 518, 3)\n",
      "Saved point cloud with 1000 points to /home/skhalid/Documents/data/banana/sparse/0/points3D.ply\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run inference here\n",
    "'''\n",
    "BATCH_SIZE=20\n",
    "SINGLE_SAMPLE = False\n",
    "normals = None\n",
    "\n",
    "predictions = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "all_extrinsics = predictions[\"all_extrinsics\"]\n",
    "all_intrinsics = predictions[\"all_intrinsics\"]\n",
    "all_world_points = predictions[\"all_world_points\"]\n",
    "depth_conf_maps = predictions[\"depth_conf_maps\"]\n",
    "\n",
    "b, h, w, c = all_world_points.shape\n",
    "images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)\n",
    "\n",
    "if SINGLE_SAMPLE:\n",
    "    all_images = load_and_preprocess_images(image_names).to(device)\n",
    "    all_images = all_images[0][None, ...]\n",
    "    # Remove batch dimension\n",
    "    world_points = all_world_points[0]  # shape: [16, 350, 518, 3]\n",
    "else:\n",
    "    all_images = load_and_preprocess_images(image_names).to(device)\n",
    "    # Remove batch dimension\n",
    "    world_points = all_world_points  # shape: [16, 350, 518, 3]\n",
    "\n",
    "print(all_images.shape)\n",
    "print(all_world_points.shape)\n",
    "\n",
    "# Flatten to [N, 3]\n",
    "points_pmap = world_points.reshape(-1, 3)  # shape: [16 * 350 * 518, 3]\n",
    "colors_pmap = all_images.permute(0,2,3,1).reshape(-1, 3).cpu().numpy()\n",
    "\n",
    "# Convert to NumPy\n",
    "points_np = points_pmap\n",
    "colors_np = colors_pmap\n",
    "\n",
    "# Optional: Remove invalid points (e.g., zero points)\n",
    "mask = np.logical_and.reduce([np.isfinite(points_np[:, 0]),\n",
    "                              np.isfinite(points_np[:, 1]),\n",
    "                              np.isfinite(points_np[:, 2])])\n",
    "points_np = points_np[mask]\n",
    "colors_np = colors_np[mask]\n",
    "\n",
    "if len(points_np) >= N:\n",
    "    indices = np.random.choice(len(points_np), N, replace=False)\n",
    "    sampled_points = points_np[indices]\n",
    "    sampled_colors = colors_np[indices]\n",
    "else:\n",
    "    print(f\"Warning: Only {len(points_np)} points available, returning all.\")\n",
    "    sampled_points = points_np\n",
    "    sampled_colors = colors_np\n",
    "\n",
    "# Save as PLY\n",
    "save_point_cloud_ply(PTS_PATH, sampled_points, sampled_colors, normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8358f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSAMPLE OBJECT SEGMENTATION (BACKUP)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SAMPLE OBJECT SEGMENTATION (BACKUP)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bf63765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model = YOLO(\"yolo11n-seg\")\n",
    "\n",
    "# # results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
    "# # results = model([\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"])\n",
    "# # results = model([\"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"])\n",
    "# results = model([\"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"])\n",
    "\n",
    "# for result in results:\n",
    "#     result.show()\n",
    "\n",
    "# # fig = plt.figure(figsize=(10,10))\n",
    "# # plt.imshow(results.render()[0])\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf205ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "# from PIL import Image\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# import clip\n",
    "# import urllib.request\n",
    "# from torchvision import transforms\n",
    "\n",
    "# # Load image\n",
    "# # image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image_rgb = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "# # Step 1: Load Segment Anything Model\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# model_type = \"vit_l\"\n",
    "\n",
    "# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "# sam.eval().cuda()\n",
    "\n",
    "# predictor = SamPredictor(sam)\n",
    "# predictor.set_image(image_rgb)\n",
    "\n",
    "# # Step 2: Predict masks with SAM (automatic mode)\n",
    "# from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "# mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "# masks = mask_generator.generate(image_rgb)\n",
    "\n",
    "# # Step 3: Load CLIP\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# # Step 4: Classify each mask region with CLIP\n",
    "# def classify_mask_with_clip(image_rgb, mask, clip_model, preprocess):\n",
    "#     # Crop the mask region\n",
    "#     # x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8)).tolist()\n",
    "#     x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8))\n",
    "#     cropped = image_rgb[y0:y1, x0:x1]\n",
    "\n",
    "#     # Apply mask\n",
    "#     masked_image = image_rgb.copy()\n",
    "#     masked_image[~mask.astype(bool)] = 0  # Black out background\n",
    "    \n",
    "#     if cropped.shape[0] < 10 or cropped.shape[1] < 10:\n",
    "#         return \"Unknown\", 0.0\n",
    "\n",
    "#     pil_crop = Image.fromarray(cropped)\n",
    "#     image_input = preprocess(pil_crop).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Define your candidate labels\n",
    "#     labels = [\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]\n",
    "#     # text_inputs = torch.cat([clip.tokenize(f\"a cropped snippet of an image extracted from the tanks and temples dataset that looks like {c}\") for c in labels]).to(device)\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"the cropped image extracted from the mipnerf360 dataset that looks like {c}\") for c in labels]).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         image_features = clip_model.encode_image(image_input)\n",
    "#         text_features = clip_model.encode_text(text_inputs)\n",
    "\n",
    "#         image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#         text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#         label_id = similarity.argmax().item()\n",
    "\n",
    "#         confidence = similarity[0, label_id].item()\n",
    "#         if confidence < 0.3:\n",
    "#             return \"Unknown\", confidence\n",
    "\n",
    "#         return labels[label_id], confidence\n",
    "\n",
    "# # Step 5: Visualize masks and CLIP labels\n",
    "# output_img = image_rgb.copy()\n",
    "# for mask_data in masks:\n",
    "#     mask = mask_data['segmentation']\n",
    "#     label, confidence = classify_mask_with_clip(image_rgb, mask, clip_model, preprocess)\n",
    "\n",
    "#     if label == \"Unknown\":\n",
    "#         continue\n",
    "# # \n",
    "#     # Draw mask\n",
    "#     color = np.random.randint(0, 255, size=3)\n",
    "#     output_img[mask] = 0.6 * output_img[mask] + 0.4 * color\n",
    "# # \n",
    "#     # Draw label\n",
    "#     y, x = np.argwhere(mask).mean(axis=0).astype(int)\n",
    "#     cv2.putText(output_img, f\"{label} {confidence:.2f}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 2)\n",
    "\n",
    "# # Show results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(output_img)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Segment Anything + CLIP classification\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b67fc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load OWL-ViT\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\").to(device)\n",
    "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "\n",
    "# # Load SAM\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# sam = sam_model_registry[\"vit_l\"](checkpoint=sam_checkpoint).to(device).eval()\n",
    "# predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57857658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load image\n",
    "# image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# image_np = np.array(image)\n",
    "\n",
    "# # Text prompt\n",
    "# # texts = [[\"car\", \"bicycle\", \"trees\", \"grass\", \"ground\", \"bench\", \"lego\", \"fruit\"]]  # You can modify this list\n",
    "# texts = [[\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]]\n",
    "\n",
    "# # Prepare input for OWL-ViT\n",
    "# inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Detect with OWL-ViT\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Get boxes and scores\n",
    "# target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "# results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.3)[0]\n",
    "\n",
    "# # Run SAM\n",
    "# predictor.set_image(image_np)\n",
    "\n",
    "# # Process each box from OWL-ViT\n",
    "# for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
    "#     box = box.cpu().numpy().astype(int)\n",
    "#     x0, y0, x1, y1 = box\n",
    "#     print(f\"{texts[0][label]}: {score:.2f} at box {box}\")\n",
    "\n",
    "#     # SAM expects box in XYXY format\n",
    "#     input_box = np.array([x0, y0, x1, y1])\n",
    "#     masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)\n",
    "\n",
    "#     # Overlay mask\n",
    "#     mask = masks[0]\n",
    "#     overlay = image_np.copy()\n",
    "#     overlay[mask] = (255, 0, 0)  # Red mask\n",
    "\n",
    "#     # Draw bounding box\n",
    "#     cv2.rectangle(overlay, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "#     cv2.putText(overlay, f\"{texts[0][label]} {score:.2f}\", (x0, y0 - 10),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # Show\n",
    "#     plt.imshow(overlay)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
