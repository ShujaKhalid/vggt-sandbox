{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e174d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGT(\n",
       "  (aggregator): Aggregator(\n",
       "    (patch_embed): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (rope): RotaryPositionEmbedding2D()\n",
       "    (frame_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (global_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (camera_head): CameraHead(\n",
       "    (trunk): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (token_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (trunk_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_pose): Linear(in_features=9, out_features=2048, bias=True)\n",
       "    (poseLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    )\n",
       "    (adaln_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "    (pose_branch): Mlp(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (point_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (track_head): TrackHead(\n",
       "    (feature_extractor): DPTHead(\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (projects): ModuleList(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (resize_layers): ModuleList(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (2): Identity()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (refinenet1): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (output_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (tracker): BaseTrackerPredictor(\n",
       "      (corr_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=567, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (updateformer): EfficientUpdateFormer(\n",
       "        (input_norm): LayerNorm((388,), eps=1e-05, elementwise_affine=True)\n",
       "        (input_transform): Linear(in_features=388, out_features=384, bias=True)\n",
       "        (output_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (flow_head): Linear(in_features=384, out_features=130, bias=True)\n",
       "        (time_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_point2virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual2point_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fmap_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffeat_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (ffeat_updater): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (vis_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (conf_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model_vggt = VGGT()\n",
    "_URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "model_vggt.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "model_vggt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0cb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE=\"bicycle\"\n",
    "SKIP=25\n",
    "\n",
    "if SCENE==\"banana\": \n",
    "    # Load and preprocess example images (replace with your own image paths)\n",
    "    image_names = [\n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00001.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00002.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00003.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00004.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00005.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00006.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00007.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00008.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00009.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00010.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00011.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00012.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00013.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00014.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00015.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00016.JPG\"\n",
    "    ]\n",
    "    ### BANANA\n",
    "    width = 3008\n",
    "    height = 2000\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/banana\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"frame_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"lego\": \n",
    "    ### LEGO\n",
    "    image_names = [\"/home/skhalid/Documents/data/nerf_synthetic/lego/train/r_\"+str(v)+\".png\" for v in range(0, 99, SKIP)]\n",
    "    width = 800\n",
    "    height = 800\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/nerf_synthetic/lego/\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"r_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"bicycle\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/360_v2/bicycle/images_4/_DSC\"\n",
    "    image_names = [BASE+str(v)+\".JPG\" for v in range(8679, 8873, SKIP)]\n",
    "    width = 1236\n",
    "    height = 821    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/360_v2/bicycle\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"_DSC\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n",
    "\n",
    "elif SCENE==\"truck\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/\"\n",
    "    image_names = [BASE+str(v).zfill(6)+\".jpg\" for v in range(1, 252, SKIP)]\n",
    "    width = 1957\n",
    "    height = 1091    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/tandt_db/tandt/truck\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e13e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b124f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_batched_camera_inference(model, image_names, batch_size=8, device='cuda', dtype=torch.float16):\n",
    "    from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "    from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "    # from vggt.utils.io import load_and_preprocess_images\n",
    "\n",
    "    all_extrinsics = []\n",
    "    all_intrinsics = []\n",
    "    all_world_points = []\n",
    "    depth_maps = []\n",
    "    depth_conf_maps = []\n",
    "    batch_tensors = []\n",
    "    agg_tokens_tensor = torch.Tensor([]).cuda()\n",
    "    ps_idx_list = []\n",
    "\n",
    "    # Batch the rest of the images\n",
    "    print(f\"Processing the rest of {len(image_names)} images in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i + batch_size]\n",
    "        batch_tensor = load_and_preprocess_images(batch_names).to(device)\n",
    "\n",
    "        if i==0:\n",
    "            first_image = batch_tensor[0]\n",
    "            print(\"first_image.shape: {}\".format(batch_tensor.shape))\n",
    "        else:\n",
    "            # Add the first reference image to this batch as well\n",
    "            batch_tensor = torch.cat((first_image[None], batch_tensor), dim=0)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
    "            batch_tensor = batch_tensor[None]  # Add batch dim\n",
    "            agg_tokens, ps_idx = model.aggregator(batch_tensor)\n",
    "\n",
    "            pose_enc = model.camera_head(agg_tokens)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, batch_tensor.shape[-2:])\n",
    "\n",
    "            depth_map, depth_conf_map = model.depth_head(agg_tokens, batch_tensor, ps_idx)\n",
    "            \n",
    "            point_map_unproj = unproject_depth_map_to_point_map(depth_map.squeeze(0), extrinsic.squeeze(0), intrinsic.squeeze(0))\n",
    "    \n",
    "            # if i==0:    \n",
    "            #     print(\"batch: {} | point_map_unproj.shape: {}\".format(i, point_map_unproj.shape))\n",
    "            # else:\n",
    "            #     print(\"batch: {} | point_map_unproj[1:, ...].shape: {}\".format(i, point_map_unproj[1:, ...].shape))\n",
    "\n",
    "            # ==== USAGE ====\n",
    "            # Inputs\n",
    "            # mask: binary torch.Tensor of shape (H, W)\n",
    "            # images: list of images (B, C, H, W) — assume 1 scene for simplicity\n",
    "            agg_tokens = torch.stack(agg_tokens)\n",
    "\n",
    "            if i==0:\n",
    "                all_extrinsics.append(extrinsic[0, ...])\n",
    "                all_intrinsics.append(intrinsic[0, ...])\n",
    "                all_world_points.append(point_map_unproj)\n",
    "                depth_maps.append(depth_map[0, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, ...])\n",
    "                batch_tensors.append(batch_tensor[0, ...])\n",
    "                agg_tokens_tensor = torch.cat((agg_tokens_tensor, agg_tokens), dim=0)\n",
    "                ps_idx_list.append(ps_idx)\n",
    "            else:\n",
    "                all_extrinsics.append(extrinsic[0, 1:])\n",
    "                all_intrinsics.append(intrinsic[0, 1:])\n",
    "                all_world_points.append(point_map_unproj[1:, ...])\n",
    "                depth_maps.append(depth_map[0, 1:, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, 1:, ...])\n",
    "                batch_tensors.append(batch_tensor[0, 1:, ...])\n",
    "                agg_tokens_tensor = torch.cat((agg_tokens_tensor, agg_tokens), dim=0)\n",
    "                ps_idx_list.append(ps_idx)\n",
    "\n",
    "            # print(\"extrinsic: {}\".format(extrinsic.shape))\n",
    "            # print(\"intrinsic: {}\".format(intrinsic.shape))\n",
    "            # print(\"point_map_unproj: {}\".format(point_map_unproj.shape))\n",
    "            # print(\"depth_map: {}\".format(depth_map.shape))\n",
    "            # print(\"depth_conf_map: {}\".format(depth_conf_map.shape))\n",
    "            # print(\"batch_tensor: {}\".format(batch_tensor.shape))\n",
    "\n",
    "    # Stack everything\n",
    "    batch_tensors = torch.cat(batch_tensors)  # [N, 4, 4]\n",
    "    all_extrinsics = torch.cat(all_extrinsics)  # [N, 4, 4]\n",
    "    all_intrinsics = torch.cat(all_intrinsics)  # [N, 3, 3]\n",
    "    all_world_points = np.concatenate(all_world_points)  # [N, H, W, 3]\n",
    "    depth_maps = torch.cat(depth_maps, dim=0)  # [N, H, W, 3]\n",
    "    depth_conf_maps = torch.cat(depth_conf_maps, dim=0)  # [N, H, W, 3]\n",
    "    # aggregated_tokens_list = torch.cat(aggregated_tokens_list, dim=0)  # [N, H, W, 3]\n",
    "    # ps_idx_list = torch.cat(ps_idx_list, dim=0)  # [N, H, W, 3]\n",
    "\n",
    "    return {\n",
    "        \"all_extrinsics\": all_extrinsics, \n",
    "        \"all_intrinsics\": all_intrinsics, \n",
    "        \"all_world_points\": all_world_points,\n",
    "        \"depth_maps\": depth_maps,\n",
    "        \"depth_conf_maps\": depth_conf_maps,\n",
    "        \"all_images\": batch_tensors,\n",
    "        \"aggregated_tokens_list\": agg_tokens_tensor,\n",
    "        \"ps_idx_list\": ps_idx_list\n",
    "    }\n",
    "\n",
    "    # # Predict Tracks\n",
    "    # # choose your own points to track, with shape (N, 2) for one scene\n",
    "    # query_points = torch.FloatTensor([[100.0, 200.0], \n",
    "    #                                     [60.72, 259.94]]).to(device)\n",
    "    # track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a370999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the rest of 8 images in batches of 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_image.shape: torch.Size([8, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90340/246345177.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[ 0.1257,  0.1242, -0.2038,  ..., -0.0134,  0.0434,  0.2083],\n",
      "          [ 0.3007,  0.1050, -0.2930,  ..., -0.0378, -0.0318,  0.1900],\n",
      "          [ 0.2889,  0.1374, -0.2838,  ..., -0.0228, -0.0320,  0.1596],\n",
      "          ...,\n",
      "          [ 0.1598,  0.0304,  0.0070,  ...,  0.0339,  0.0843,  0.0852],\n",
      "          [ 0.2833,  0.2390, -0.0607,  ...,  0.0203,  0.0170,  0.1502],\n",
      "          [ 0.1142,  0.0103, -0.0572,  ...,  0.0736,  0.0642,  0.1011]],\n",
      "\n",
      "         [[ 0.0821,  0.1083, -0.0273,  ...,  0.1338,  0.0524,  0.0194],\n",
      "          [ 0.0764,  0.0978, -0.0471,  ...,  0.1553,  0.0553,  0.0360],\n",
      "          [ 0.0829,  0.0318, -0.0437,  ...,  0.0838, -0.0308,  0.0934],\n",
      "          ...,\n",
      "          [ 0.0960, -0.0321, -0.0784,  ...,  0.1196, -0.0677,  0.0835],\n",
      "          [-0.0761,  0.1387,  0.0207,  ...,  0.2146,  0.1693,  0.0715],\n",
      "          [ 0.0262, -0.0612, -0.1492,  ...,  0.1321,  0.0060,  0.1199]],\n",
      "\n",
      "         [[ 0.2074,  0.0754, -0.1057,  ..., -0.0731,  0.0325,  0.1814],\n",
      "          [ 0.3363, -0.0595, -0.1242,  ..., -0.0532,  0.0385,  0.1013],\n",
      "          [ 0.2773,  0.1217,  0.0711,  ..., -0.0389,  0.0124,  0.1126],\n",
      "          ...,\n",
      "          [ 0.2319, -0.1147, -0.0143,  ..., -0.0088, -0.0334,  0.0497],\n",
      "          [ 0.1253,  0.0298,  0.0080,  ...,  0.0062,  0.0475,  0.1213],\n",
      "          [ 0.0940,  0.0623, -0.0752,  ...,  0.0099,  0.0708,  0.0805]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1446,  0.0971, -0.1137,  ...,  0.0279,  0.0337,  0.1403],\n",
      "          [ 0.1619,  0.0316, -0.1627,  ...,  0.0740,  0.0170,  0.0647],\n",
      "          [ 0.2571, -0.1141,  0.0919,  ...,  0.0424,  0.0139,  0.0589],\n",
      "          ...,\n",
      "          [ 0.1663,  0.1332,  0.1165,  ...,  0.0448,  0.0015,  0.2691],\n",
      "          [ 0.1031,  0.1949, -0.0230,  ...,  0.1144, -0.0149,  0.2086],\n",
      "          [ 0.1234,  0.1461, -0.0923,  ...,  0.1068, -0.0503,  0.1901]],\n",
      "\n",
      "         [[ 0.2550, -0.1257, -0.1518,  ...,  0.1245,  0.0033,  0.0267],\n",
      "          [ 0.0985,  0.1112, -0.2342,  ...,  0.1130,  0.0078,  0.1026],\n",
      "          [ 0.1534, -0.0607, -0.1688,  ...,  0.0727,  0.0722,  0.1442],\n",
      "          ...,\n",
      "          [ 0.2232, -0.0179, -0.1571,  ...,  0.0717,  0.0535,  0.0242],\n",
      "          [ 0.2637, -0.0921, -0.1157,  ...,  0.0759, -0.0634, -0.0665],\n",
      "          [ 0.2232,  0.0090, -0.1331,  ...,  0.1240,  0.0646,  0.0345]],\n",
      "\n",
      "         [[ 0.1931,  0.0393, -0.0936,  ..., -0.0318,  0.0790,  0.0783],\n",
      "          [ 0.2496,  0.1412, -0.0555,  ...,  0.0211,  0.0140,  0.0998],\n",
      "          [ 0.1623,  0.1743, -0.0463,  ...,  0.0471, -0.0236,  0.1747],\n",
      "          ...,\n",
      "          [ 0.3147,  0.1329, -0.0675,  ..., -0.0571,  0.0145,  0.0535],\n",
      "          [ 0.2332,  0.1649, -0.1004,  ..., -0.0215,  0.0634,  0.0982],\n",
      "          [ 0.1708,  0.1870, -0.0976,  ...,  0.0374,  0.0471,  0.1043]]]],\n",
      "       device='cuda:0')\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[ 0.0874,  0.3313, -0.2651,  ..., -0.1093, -0.1747,  0.3488],\n",
      "          [-0.0924,  0.2550, -0.4052,  ..., -0.0282, -0.0842,  0.3460],\n",
      "          [-0.1335,  0.3280, -0.3135,  ...,  0.1337, -0.0742,  0.2686],\n",
      "          ...,\n",
      "          [-0.0011,  0.2588,  0.0419,  ..., -0.0729, -0.0300,  0.1533],\n",
      "          [ 0.1531,  0.5514,  0.0248,  ..., -0.0076, -0.1028,  0.1123],\n",
      "          [ 0.0276,  0.2072, -0.0366,  ..., -0.1054, -0.2999,  0.1406]],\n",
      "\n",
      "         [[ 0.1057,  0.2429, -0.0929,  ..., -0.1079, -0.1767,  0.2383],\n",
      "          [ 0.0611,  0.2082, -0.1348,  ..., -0.0596, -0.1004,  0.2943],\n",
      "          [ 0.0194,  0.1103, -0.1472,  ..., -0.1304, -0.2521,  0.3513],\n",
      "          ...,\n",
      "          [ 0.0056,  0.1345, -0.1000,  ...,  0.1275,  0.0803,  0.0100],\n",
      "          [-0.0581,  0.3129,  0.0595,  ...,  0.0313,  0.1216, -0.1288],\n",
      "          [-0.0064,  0.1765, -0.1634,  ...,  0.0560,  0.0815, -0.0361]],\n",
      "\n",
      "         [[ 0.1279,  0.2909, -0.2145,  ..., -0.1610, -0.1963,  0.3781],\n",
      "          [ 0.1066,  0.1779, -0.1621,  ..., -0.2354, -0.0717,  0.2989],\n",
      "          [-0.0144,  0.2546, -0.0215,  ..., -0.0794,  0.0523,  0.3805],\n",
      "          ...,\n",
      "          [ 0.1303,  0.0815, -0.1264,  ...,  0.0953, -0.2146,  0.0243],\n",
      "          [ 0.0572,  0.2112, -0.0578,  ..., -0.0440, -0.1679,  0.1092],\n",
      "          [ 0.0322,  0.2779, -0.0988,  ..., -0.1138, -0.1957,  0.0611]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1389,  0.1529, -0.1209,  ..., -0.1807, -0.2988,  0.3729],\n",
      "          [ 0.1053,  0.1865,  0.0262,  ..., -0.0426, -0.0707,  0.2440],\n",
      "          [-0.0521, -0.0040,  0.2156,  ...,  0.0451, -0.0905,  0.1398],\n",
      "          ...,\n",
      "          [ 0.2280,  0.3436,  0.0177,  ..., -0.0227, -0.1339,  0.2369],\n",
      "          [ 0.0411,  0.3936,  0.0439,  ...,  0.0084, -0.2504,  0.1734],\n",
      "          [ 0.0698,  0.3168, -0.0085,  ..., -0.0356, -0.3411,  0.0982]],\n",
      "\n",
      "         [[ 0.2164,  0.1561, -0.1286,  ..., -0.0910, -0.1405,  0.2628],\n",
      "          [ 0.1433,  0.3472, -0.2547,  ..., -0.2845, -0.1781,  0.1820],\n",
      "          [-0.1337,  0.2209, -0.0781,  ..., -0.0095, -0.0087,  0.1037],\n",
      "          ...,\n",
      "          [ 0.0704,  0.2683, -0.1343,  ...,  0.1315, -0.0805, -0.0197],\n",
      "          [ 0.1786,  0.2332, -0.0293,  ...,  0.1201, -0.1261, -0.0253],\n",
      "          [ 0.2811,  0.2319, -0.0971,  ..., -0.0010, -0.1379,  0.1974]],\n",
      "\n",
      "         [[ 0.1251,  0.1964, -0.0298,  ..., -0.1476, -0.0045,  0.0950],\n",
      "          [ 0.1693,  0.3111,  0.1184,  ..., -0.0010,  0.0092,  0.0099],\n",
      "          [ 0.0313,  0.3360,  0.0741,  ..., -0.0328, -0.0978,  0.1659],\n",
      "          ...,\n",
      "          [ 0.2071,  0.3537, -0.1203,  ...,  0.0025, -0.1372,  0.1515],\n",
      "          [ 0.0319,  0.3472, -0.1284,  ..., -0.1899, -0.2334,  0.1658],\n",
      "          [-0.0526,  0.3706, -0.0957,  ..., -0.1931, -0.2815,  0.1397]]]],\n",
      "       device='cuda:0')\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[-6.3289e-02,  8.4456e-01,  1.1943e-01,  ..., -4.5010e-01,\n",
      "           -8.4542e-01,  7.1680e-03],\n",
      "          [ 1.1339e-01,  2.2299e+00,  1.5207e+00,  ...,  2.4798e-01,\n",
      "            6.8109e-02,  1.5803e-01],\n",
      "          [-2.3376e-02,  2.3692e+00,  1.7397e+00,  ...,  6.1709e-01,\n",
      "            1.7691e-01, -5.9742e-02],\n",
      "          ...,\n",
      "          [-3.2176e-01, -2.2821e+00, -1.6188e+00,  ..., -2.3080e-01,\n",
      "           -9.6997e-02,  1.0217e-01],\n",
      "          [-3.5414e-01, -2.5649e+00, -1.8983e+00,  ...,  8.5146e-03,\n",
      "           -4.9760e-01, -2.1576e-01],\n",
      "          [-1.6996e-01, -2.2723e+00, -1.8266e+00,  ..., -2.2958e-01,\n",
      "           -2.1712e-01,  1.2437e-03]],\n",
      "\n",
      "         [[ 1.4938e-01, -4.4594e-01, -1.0403e-01,  ..., -1.1901e+00,\n",
      "            1.1059e-01,  4.4656e-01],\n",
      "          [ 2.0005e-01, -3.9999e-01, -2.6934e-01,  ..., -9.1091e-01,\n",
      "           -6.7945e-02,  4.6279e-01],\n",
      "          [ 4.1688e-01, -5.1268e-01, -3.3719e-01,  ..., -8.8539e-01,\n",
      "           -2.4336e-01,  6.2213e-01],\n",
      "          ...,\n",
      "          [ 6.4280e-01,  5.8845e-02,  6.8239e-01,  ...,  2.8902e-01,\n",
      "            4.0826e-01,  3.3267e-01],\n",
      "          [-4.5325e-01,  2.3536e-01,  2.5518e-01,  ..., -1.5842e+00,\n",
      "           -5.5961e-01,  5.1217e-01],\n",
      "          [ 1.5090e-01,  4.2911e-01,  8.2676e-01,  ..., -2.6535e-01,\n",
      "           -4.4291e-02, -3.2281e-01]],\n",
      "\n",
      "         [[-2.1476e-01,  1.3453e+00,  4.4884e-01,  ..., -4.9024e-01,\n",
      "           -1.2358e+00,  1.7760e-01],\n",
      "          [-2.1586e-01,  2.8063e+00,  2.2161e+00,  ...,  1.9243e-01,\n",
      "           -8.4453e-02, -2.8474e-01],\n",
      "          [-2.2572e-01,  2.5264e+00,  2.1352e+00,  ...,  1.5497e-01,\n",
      "           -8.0277e-01, -2.3809e-02],\n",
      "          ...,\n",
      "          [-3.9052e-01, -3.4168e+00, -2.0017e+00,  ...,  3.1656e-01,\n",
      "            1.4543e-01,  2.5517e-01],\n",
      "          [-3.9974e-01, -2.4044e+00, -1.4836e+00,  ...,  3.0207e-01,\n",
      "           -2.2120e-01,  9.9804e-02],\n",
      "          [-3.3565e-01, -2.2663e+00, -1.5837e+00,  ...,  2.2149e-01,\n",
      "           -1.3301e-01, -1.3436e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3278e-01, -3.6045e-01, -1.1370e+00,  ..., -4.3049e-01,\n",
      "           -2.7702e-01, -1.3782e+00],\n",
      "          [-3.3142e-02, -9.0086e-01, -3.7385e-01,  ...,  2.7028e-01,\n",
      "            2.0320e-01, -1.5658e+00],\n",
      "          [-1.5735e-01, -8.3676e-01, -4.1904e-02,  ...,  1.8125e-01,\n",
      "           -3.2676e-01, -1.7704e+00],\n",
      "          ...,\n",
      "          [-4.0104e-01,  1.3427e-01,  3.0421e-01,  ..., -1.3666e-01,\n",
      "           -2.0703e-02,  4.9629e-01],\n",
      "          [-4.8618e-01,  5.6126e-01,  4.4888e-01,  ..., -5.2465e-01,\n",
      "            4.9557e-02,  5.8741e-01],\n",
      "          [-3.5467e-01,  3.4842e-01,  2.4717e-01,  ..., -3.7518e-01,\n",
      "            5.6113e-02,  3.4541e-01]],\n",
      "\n",
      "         [[ 5.6051e-01,  1.7390e+00,  1.1793e+00,  ..., -2.0649e-01,\n",
      "            1.1399e-01, -6.9520e-02],\n",
      "          [ 1.7940e-01,  1.1081e+00,  2.9595e-01,  ..., -9.3686e-01,\n",
      "           -6.0198e-01,  1.2316e-01],\n",
      "          [ 5.6801e-01,  1.2887e+00,  7.6268e-01,  ..., -5.3883e-01,\n",
      "            8.0207e-01, -6.4096e-01],\n",
      "          ...,\n",
      "          [ 3.2395e-01, -8.6910e-01, -1.2533e+00,  ...,  4.3327e-01,\n",
      "           -5.0208e-02,  1.6928e-01],\n",
      "          [ 3.5543e-01, -9.3007e-01, -1.1551e+00,  ...,  8.6482e-01,\n",
      "           -2.8278e-01,  1.6601e-01],\n",
      "          [-3.8553e-02, -6.1234e-01, -7.1148e-01,  ..., -2.2919e-01,\n",
      "           -8.9462e-01,  1.2913e+00]],\n",
      "\n",
      "         [[-1.0240e-01,  8.5114e-01,  1.1398e+00,  ..., -6.8985e-01,\n",
      "           -3.3074e-01, -5.8245e-01],\n",
      "          [ 2.8520e-01,  9.5164e-01,  1.9384e+00,  ..., -2.8128e-01,\n",
      "           -5.9496e-01, -3.3918e-01],\n",
      "          [-4.6677e-02,  1.0173e+00,  1.6954e+00,  ..., -4.7169e-01,\n",
      "           -3.9436e-01, -3.4464e-01],\n",
      "          ...,\n",
      "          [-4.6012e-02, -7.2739e-01, -8.2055e-01,  ...,  3.2047e-02,\n",
      "           -1.3377e-01,  4.1145e-01],\n",
      "          [-1.9049e-01, -4.7534e-01, -7.2514e-01,  ..., -2.1277e-01,\n",
      "           -2.9610e-01,  3.6651e-01],\n",
      "          [-2.4270e-01, -3.3445e-01, -7.0966e-01,  ..., -1.8586e-01,\n",
      "           -1.5720e-01,  4.6546e-01]]]], device='cuda:0')\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[ 0.5244,  1.8721,  0.6143,  ..., -0.9419, -0.4883, -1.8272],\n",
      "          [ 0.5051,  3.2486,  1.8848,  ..., -0.2367,  0.7393, -1.2829],\n",
      "          [ 0.2638,  3.2657,  2.0418,  ...,  0.0207,  0.7029, -1.5195],\n",
      "          ...,\n",
      "          [ 0.9885, -1.2110, -1.1610,  ..., -1.1070, -0.4766, -1.4052],\n",
      "          [ 1.0403, -1.3374, -1.3384,  ..., -0.7547, -0.7343, -1.6351],\n",
      "          [ 0.9755, -1.1852, -1.3169,  ..., -1.3520, -0.6852, -1.4287]],\n",
      "\n",
      "         [[ 0.1171,  0.5199, -0.3760,  ..., -0.8269, -0.3298, -0.9556],\n",
      "          [ 0.2504,  0.8076, -0.3937,  ..., -0.8191, -0.2015, -0.5212],\n",
      "          [ 0.7632,  0.8735, -0.3086,  ..., -0.8285, -0.0178, -0.0621],\n",
      "          ...,\n",
      "          [-0.6823,  0.6225,  0.6321,  ...,  3.0017, -0.6021, -1.7760],\n",
      "          [-1.5873,  1.2132,  0.5204,  ..., -1.5695, -1.5665, -0.5651],\n",
      "          [-0.9977,  0.8786,  0.5768,  ...,  1.7739, -1.1600, -2.1065]],\n",
      "\n",
      "         [[ 0.1583,  2.6542,  1.2219,  ..., -0.9643, -0.9958, -2.2093],\n",
      "          [ 0.2749,  4.0883,  2.9732,  ..., -0.3651,  0.3046, -2.1806],\n",
      "          [ 0.4011,  3.8683,  2.8706,  ..., -0.7675, -0.3213, -1.8485],\n",
      "          ...,\n",
      "          [ 0.8048, -2.2161, -1.2137,  ..., -0.1473,  0.3556, -1.5552],\n",
      "          [ 0.8557, -1.3582, -0.8687,  ..., -0.1011, -0.1126, -1.8937],\n",
      "          [ 0.9428, -1.2280, -0.9646,  ..., -0.2565, -0.0784, -2.0344]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1409,  0.5452, -2.3071,  ...,  1.0499,  1.4547, -1.4422],\n",
      "          [ 1.7600,  0.2766, -2.0613,  ...,  2.4385,  1.4416, -1.3818],\n",
      "          [ 0.9318,  0.0611, -1.7670,  ...,  1.8131,  0.9772, -1.7055],\n",
      "          ...,\n",
      "          [-0.1856,  1.2957,  1.4098,  ..., -1.5759,  0.3535, -1.2459],\n",
      "          [ 0.2431,  1.6823,  1.3662,  ..., -1.9389,  0.4282, -0.8320],\n",
      "          [ 0.1741,  1.4913,  1.1322,  ..., -1.7967,  0.3515, -1.0028]],\n",
      "\n",
      "         [[ 1.8105,  2.6517,  1.6006,  ..., -1.3015,  0.2489, -1.2351],\n",
      "          [ 1.2455,  2.2054,  1.0354,  ..., -1.9106,  0.0220, -1.5798],\n",
      "          [ 2.0130,  2.2045,  1.3195,  ..., -2.1543,  1.2088, -2.0332],\n",
      "          ...,\n",
      "          [ 0.8008, -0.0542, -1.0230,  ..., -0.6252,  0.0982, -0.6408],\n",
      "          [ 1.0512, -0.1368, -1.0008,  ..., -0.2273, -0.1953, -0.6769],\n",
      "          [ 0.2636,  0.1235, -0.8421,  ..., -0.2527, -0.1537, -0.1211]],\n",
      "\n",
      "         [[ 0.4416,  1.9277,  1.2598,  ...,  1.0158,  0.6545, -1.5941],\n",
      "          [ 0.4499,  1.8662,  1.9155,  ...,  0.9847,  0.5724, -1.4549],\n",
      "          [ 0.1191,  2.3348,  1.9584,  ...,  0.6582,  0.9331, -1.5098],\n",
      "          ...,\n",
      "          [ 1.1476,  0.4234, -0.0798,  ..., -0.5628,  0.1077, -1.2007],\n",
      "          [ 1.1146,  0.6258, -0.0542,  ..., -0.9200, -0.2859, -1.2362],\n",
      "          [ 1.2390,  0.7390, -0.0752,  ..., -0.8602, -0.2693, -1.0987]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=10\n",
    "\n",
    "predictions = run_batched_camera_inference(model_vggt, image_names, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7117a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 4) (8, 3, 3) (8, 350, 518, 3) (8, 350, 518, 1) (8, 350, 518) (8, 3, 350, 518) torch.Size([24, 1, 8, 930, 2048])\n"
     ]
    }
   ],
   "source": [
    "all_extrinsics = predictions[\"all_extrinsics\"].cpu().numpy()\n",
    "all_intrinsics = predictions[\"all_intrinsics\"].cpu().numpy()\n",
    "all_world_points = predictions[\"all_world_points\"]\n",
    "depth_maps = predictions[\"depth_maps\"].cpu().numpy()\n",
    "depth_conf_maps = predictions[\"depth_conf_maps\"].cpu().numpy()\n",
    "all_images = predictions[\"all_images\"].cpu().numpy()\n",
    "ps_idx_list = predictions[\"ps_idx_list\"]\n",
    "aggregated_tokens_tensor = predictions[\"aggregated_tokens_list\"]\n",
    "# ps_idx_list = predictions[\"ps_idx_list\"].cpu().numpy()\n",
    "\n",
    "print(all_extrinsics.shape, all_intrinsics.shape, all_world_points.shape, depth_maps.shape, depth_conf_maps.shape, all_images.shape, aggregated_tokens_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9663664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42f5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def mask_to_query_points(mask, max_points=100):\n",
    "    \"\"\"\n",
    "    Convert a binary mask to a list of query points (N, 2)\n",
    "    \"\"\"\n",
    "    mask = mask.astype(np.uint8)\n",
    "    coords = np.column_stack(np.where(mask > 0))  # shape: (N, 2) as (row, col)\n",
    "    if coords.shape[0] == 0:\n",
    "        raise ValueError(\"No foreground pixels found in mask.\")\n",
    "    \n",
    "    # Subsample if too many points\n",
    "    if coords.shape[0] > max_points:\n",
    "        idx = np.random.choice(len(coords), max_points, replace=False)\n",
    "        coords = coords[idx]\n",
    "\n",
    "    # Flip from (row, col) to (x, y)\n",
    "    query_points = torch.FloatTensor(coords[:, [1, 0]])  # (N, 2)\n",
    "    return query_points\n",
    "\n",
    "#track_list: torch.Size([4, 1, 8, 50, 2])\n",
    "def visualize_tracks(all_images, track_list, ps_idx_list, radius=2):\n",
    "    \"\"\"\n",
    "    Visualize tracked points on image sequence.\n",
    "    \n",
    "    all_images: (T, 3, H, W) NumPy array\n",
    "    track_list: (T, 1, N, 2) torch.Tensor\n",
    "    ps_idx_list: (N, 2) torch.Tensor or NumPy array\n",
    "    \"\"\"\n",
    "    T = all_images.shape[0]\n",
    "    N = ps_idx_list.shape[0]\n",
    "\n",
    "    # Normalize images if necessary\n",
    "    if all_images.max() <= 1.0:\n",
    "        all_images = (all_images * 255).astype(np.uint8)\n",
    "    else:\n",
    "        all_images = all_images.astype(np.uint8)\n",
    "\n",
    "    # Generate unique colors\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, N))[:, :3] * 255\n",
    "\n",
    "    for t in range(T):\n",
    "        # Convert image to (H, W, 3) and transpose channels\n",
    "        img = np.transpose(all_images[t], (1, 2, 0)).copy()  # (H, W, 3)\n",
    "\n",
    "        for i in range(N):\n",
    "            point = track_list[1, 0, t, i].detach().cpu().numpy()\n",
    "            if np.any(np.isnan(point)) or np.any(np.isinf(point)):\n",
    "                continue\n",
    "            # x, y = int(round(point[0])), int(round(point[1]))\n",
    "            x, y = int(round(float(point[0]))), int(round(float(point[1])))\n",
    "            color = tuple(map(int, colors[i]))\n",
    "            cv2.circle(img, (x, y), radius, color, -1)\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Frame {t}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b71db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skhalid/Documents/vggt/venv/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/skhalid/Documents/vggt/venv/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OBJECT_MASKING\n",
    "'''\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load OWL-ViT\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\").to(device)\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "\n",
    "# Load SAM\n",
    "sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "sam = sam_model_registry[\"vit_l\"](checkpoint=sam_checkpoint).to(device).eval()\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10cf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycle: 0.71 at box [123  70 399 246]\n",
      "query_points.shape: torch.Size([100, 2])\n",
      "all_images.shape: (8, 3, 350, 518)\n",
      "ps_idx_list: [5]\n",
      "aggregated_tokens_tensor: torch.Size([24, 1, 8, 930, 2048])\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[ 0.1257,  0.1242, -0.2038,  ..., -0.0134,  0.0434,  0.2083],\n",
      "          [ 0.3007,  0.1050, -0.2930,  ..., -0.0378, -0.0318,  0.1900],\n",
      "          [ 0.2889,  0.1374, -0.2838,  ..., -0.0228, -0.0320,  0.1596],\n",
      "          ...,\n",
      "          [ 0.1598,  0.0304,  0.0070,  ...,  0.0339,  0.0843,  0.0852],\n",
      "          [ 0.2833,  0.2390, -0.0607,  ...,  0.0203,  0.0170,  0.1502],\n",
      "          [ 0.1142,  0.0103, -0.0572,  ...,  0.0736,  0.0642,  0.1011]],\n",
      "\n",
      "         [[ 0.0821,  0.1083, -0.0273,  ...,  0.1338,  0.0524,  0.0194],\n",
      "          [ 0.0764,  0.0978, -0.0471,  ...,  0.1553,  0.0553,  0.0360],\n",
      "          [ 0.0829,  0.0318, -0.0437,  ...,  0.0838, -0.0308,  0.0934],\n",
      "          ...,\n",
      "          [ 0.0960, -0.0321, -0.0784,  ...,  0.1196, -0.0677,  0.0835],\n",
      "          [-0.0761,  0.1387,  0.0207,  ...,  0.2146,  0.1693,  0.0715],\n",
      "          [ 0.0262, -0.0612, -0.1492,  ...,  0.1321,  0.0060,  0.1199]],\n",
      "\n",
      "         [[ 0.2074,  0.0754, -0.1057,  ..., -0.0731,  0.0325,  0.1814],\n",
      "          [ 0.3363, -0.0595, -0.1242,  ..., -0.0532,  0.0385,  0.1013],\n",
      "          [ 0.2773,  0.1217,  0.0711,  ..., -0.0389,  0.0124,  0.1126],\n",
      "          ...,\n",
      "          [ 0.2319, -0.1147, -0.0143,  ..., -0.0088, -0.0334,  0.0497],\n",
      "          [ 0.1253,  0.0298,  0.0080,  ...,  0.0062,  0.0475,  0.1213],\n",
      "          [ 0.0940,  0.0623, -0.0752,  ...,  0.0099,  0.0708,  0.0805]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1446,  0.0971, -0.1137,  ...,  0.0279,  0.0337,  0.1403],\n",
      "          [ 0.1619,  0.0316, -0.1627,  ...,  0.0740,  0.0170,  0.0647],\n",
      "          [ 0.2571, -0.1141,  0.0919,  ...,  0.0424,  0.0139,  0.0589],\n",
      "          ...,\n",
      "          [ 0.1663,  0.1332,  0.1165,  ...,  0.0448,  0.0015,  0.2691],\n",
      "          [ 0.1031,  0.1949, -0.0230,  ...,  0.1144, -0.0149,  0.2086],\n",
      "          [ 0.1234,  0.1461, -0.0923,  ...,  0.1068, -0.0503,  0.1901]],\n",
      "\n",
      "         [[ 0.2550, -0.1257, -0.1518,  ...,  0.1245,  0.0033,  0.0267],\n",
      "          [ 0.0985,  0.1112, -0.2342,  ...,  0.1130,  0.0078,  0.1026],\n",
      "          [ 0.1534, -0.0607, -0.1688,  ...,  0.0727,  0.0722,  0.1442],\n",
      "          ...,\n",
      "          [ 0.2232, -0.0179, -0.1571,  ...,  0.0717,  0.0535,  0.0242],\n",
      "          [ 0.2637, -0.0921, -0.1157,  ...,  0.0759, -0.0634, -0.0665],\n",
      "          [ 0.2232,  0.0090, -0.1331,  ...,  0.1240,  0.0646,  0.0345]],\n",
      "\n",
      "         [[ 0.1931,  0.0393, -0.0936,  ..., -0.0318,  0.0790,  0.0783],\n",
      "          [ 0.2496,  0.1412, -0.0555,  ...,  0.0211,  0.0140,  0.0998],\n",
      "          [ 0.1623,  0.1743, -0.0463,  ...,  0.0471, -0.0236,  0.1747],\n",
      "          ...,\n",
      "          [ 0.3147,  0.1329, -0.0675,  ..., -0.0571,  0.0145,  0.0535],\n",
      "          [ 0.2332,  0.1649, -0.1004,  ..., -0.0215,  0.0634,  0.0982],\n",
      "          [ 0.1708,  0.1870, -0.0976,  ...,  0.0374,  0.0471,  0.1043]]]],\n",
      "       device='cuda:0')\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[ 0.0874,  0.3313, -0.2651,  ..., -0.1093, -0.1747,  0.3488],\n",
      "          [-0.0924,  0.2550, -0.4052,  ..., -0.0282, -0.0842,  0.3460],\n",
      "          [-0.1335,  0.3280, -0.3135,  ...,  0.1337, -0.0742,  0.2686],\n",
      "          ...,\n",
      "          [-0.0011,  0.2588,  0.0419,  ..., -0.0729, -0.0300,  0.1533],\n",
      "          [ 0.1531,  0.5514,  0.0248,  ..., -0.0076, -0.1028,  0.1123],\n",
      "          [ 0.0276,  0.2072, -0.0366,  ..., -0.1054, -0.2999,  0.1406]],\n",
      "\n",
      "         [[ 0.1057,  0.2429, -0.0929,  ..., -0.1079, -0.1767,  0.2383],\n",
      "          [ 0.0611,  0.2082, -0.1348,  ..., -0.0596, -0.1004,  0.2943],\n",
      "          [ 0.0194,  0.1103, -0.1472,  ..., -0.1304, -0.2521,  0.3513],\n",
      "          ...,\n",
      "          [ 0.0056,  0.1345, -0.1000,  ...,  0.1275,  0.0803,  0.0100],\n",
      "          [-0.0581,  0.3129,  0.0595,  ...,  0.0313,  0.1216, -0.1288],\n",
      "          [-0.0064,  0.1765, -0.1634,  ...,  0.0560,  0.0815, -0.0361]],\n",
      "\n",
      "         [[ 0.1279,  0.2909, -0.2145,  ..., -0.1610, -0.1963,  0.3781],\n",
      "          [ 0.1066,  0.1779, -0.1621,  ..., -0.2354, -0.0717,  0.2989],\n",
      "          [-0.0144,  0.2546, -0.0215,  ..., -0.0794,  0.0523,  0.3805],\n",
      "          ...,\n",
      "          [ 0.1303,  0.0815, -0.1264,  ...,  0.0953, -0.2146,  0.0243],\n",
      "          [ 0.0572,  0.2112, -0.0578,  ..., -0.0440, -0.1679,  0.1092],\n",
      "          [ 0.0322,  0.2779, -0.0988,  ..., -0.1138, -0.1957,  0.0611]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1389,  0.1529, -0.1209,  ..., -0.1807, -0.2988,  0.3729],\n",
      "          [ 0.1053,  0.1865,  0.0262,  ..., -0.0426, -0.0707,  0.2440],\n",
      "          [-0.0521, -0.0040,  0.2156,  ...,  0.0451, -0.0905,  0.1398],\n",
      "          ...,\n",
      "          [ 0.2280,  0.3436,  0.0177,  ..., -0.0227, -0.1339,  0.2369],\n",
      "          [ 0.0411,  0.3936,  0.0439,  ...,  0.0084, -0.2504,  0.1734],\n",
      "          [ 0.0698,  0.3168, -0.0085,  ..., -0.0356, -0.3411,  0.0982]],\n",
      "\n",
      "         [[ 0.2164,  0.1561, -0.1286,  ..., -0.0910, -0.1405,  0.2628],\n",
      "          [ 0.1433,  0.3472, -0.2547,  ..., -0.2845, -0.1781,  0.1820],\n",
      "          [-0.1337,  0.2209, -0.0781,  ..., -0.0095, -0.0087,  0.1037],\n",
      "          ...,\n",
      "          [ 0.0704,  0.2683, -0.1343,  ...,  0.1315, -0.0805, -0.0197],\n",
      "          [ 0.1786,  0.2332, -0.0293,  ...,  0.1201, -0.1261, -0.0253],\n",
      "          [ 0.2811,  0.2319, -0.0971,  ..., -0.0010, -0.1379,  0.1974]],\n",
      "\n",
      "         [[ 0.1251,  0.1964, -0.0298,  ..., -0.1476, -0.0045,  0.0950],\n",
      "          [ 0.1693,  0.3111,  0.1184,  ..., -0.0010,  0.0092,  0.0099],\n",
      "          [ 0.0313,  0.3360,  0.0741,  ..., -0.0328, -0.0978,  0.1659],\n",
      "          ...,\n",
      "          [ 0.2071,  0.3537, -0.1203,  ...,  0.0025, -0.1372,  0.1515],\n",
      "          [ 0.0319,  0.3472, -0.1284,  ..., -0.1899, -0.2334,  0.1658],\n",
      "          [-0.0526,  0.3706, -0.0957,  ..., -0.1931, -0.2815,  0.1397]]]],\n",
      "       device='cuda:0')\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[-6.3289e-02,  8.4456e-01,  1.1943e-01,  ..., -4.5010e-01,\n",
      "           -8.4542e-01,  7.1680e-03],\n",
      "          [ 1.1339e-01,  2.2299e+00,  1.5207e+00,  ...,  2.4798e-01,\n",
      "            6.8109e-02,  1.5803e-01],\n",
      "          [-2.3376e-02,  2.3692e+00,  1.7397e+00,  ...,  6.1709e-01,\n",
      "            1.7691e-01, -5.9742e-02],\n",
      "          ...,\n",
      "          [-3.2176e-01, -2.2821e+00, -1.6188e+00,  ..., -2.3080e-01,\n",
      "           -9.6997e-02,  1.0217e-01],\n",
      "          [-3.5414e-01, -2.5649e+00, -1.8983e+00,  ...,  8.5146e-03,\n",
      "           -4.9760e-01, -2.1576e-01],\n",
      "          [-1.6996e-01, -2.2723e+00, -1.8266e+00,  ..., -2.2958e-01,\n",
      "           -2.1712e-01,  1.2437e-03]],\n",
      "\n",
      "         [[ 1.4938e-01, -4.4594e-01, -1.0403e-01,  ..., -1.1901e+00,\n",
      "            1.1059e-01,  4.4656e-01],\n",
      "          [ 2.0005e-01, -3.9999e-01, -2.6934e-01,  ..., -9.1091e-01,\n",
      "           -6.7945e-02,  4.6279e-01],\n",
      "          [ 4.1688e-01, -5.1268e-01, -3.3719e-01,  ..., -8.8539e-01,\n",
      "           -2.4336e-01,  6.2213e-01],\n",
      "          ...,\n",
      "          [ 6.4280e-01,  5.8845e-02,  6.8239e-01,  ...,  2.8902e-01,\n",
      "            4.0826e-01,  3.3267e-01],\n",
      "          [-4.5325e-01,  2.3536e-01,  2.5518e-01,  ..., -1.5842e+00,\n",
      "           -5.5961e-01,  5.1217e-01],\n",
      "          [ 1.5090e-01,  4.2911e-01,  8.2676e-01,  ..., -2.6535e-01,\n",
      "           -4.4291e-02, -3.2281e-01]],\n",
      "\n",
      "         [[-2.1476e-01,  1.3453e+00,  4.4884e-01,  ..., -4.9024e-01,\n",
      "           -1.2358e+00,  1.7760e-01],\n",
      "          [-2.1586e-01,  2.8063e+00,  2.2161e+00,  ...,  1.9243e-01,\n",
      "           -8.4453e-02, -2.8474e-01],\n",
      "          [-2.2572e-01,  2.5264e+00,  2.1352e+00,  ...,  1.5497e-01,\n",
      "           -8.0277e-01, -2.3809e-02],\n",
      "          ...,\n",
      "          [-3.9052e-01, -3.4168e+00, -2.0017e+00,  ...,  3.1656e-01,\n",
      "            1.4543e-01,  2.5517e-01],\n",
      "          [-3.9974e-01, -2.4044e+00, -1.4836e+00,  ...,  3.0207e-01,\n",
      "           -2.2120e-01,  9.9804e-02],\n",
      "          [-3.3565e-01, -2.2663e+00, -1.5837e+00,  ...,  2.2149e-01,\n",
      "           -1.3301e-01, -1.3436e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3278e-01, -3.6045e-01, -1.1370e+00,  ..., -4.3049e-01,\n",
      "           -2.7702e-01, -1.3782e+00],\n",
      "          [-3.3142e-02, -9.0086e-01, -3.7385e-01,  ...,  2.7028e-01,\n",
      "            2.0320e-01, -1.5658e+00],\n",
      "          [-1.5735e-01, -8.3676e-01, -4.1904e-02,  ...,  1.8125e-01,\n",
      "           -3.2676e-01, -1.7704e+00],\n",
      "          ...,\n",
      "          [-4.0104e-01,  1.3427e-01,  3.0421e-01,  ..., -1.3666e-01,\n",
      "           -2.0703e-02,  4.9629e-01],\n",
      "          [-4.8618e-01,  5.6126e-01,  4.4888e-01,  ..., -5.2465e-01,\n",
      "            4.9557e-02,  5.8741e-01],\n",
      "          [-3.5467e-01,  3.4842e-01,  2.4717e-01,  ..., -3.7518e-01,\n",
      "            5.6113e-02,  3.4541e-01]],\n",
      "\n",
      "         [[ 5.6051e-01,  1.7390e+00,  1.1793e+00,  ..., -2.0649e-01,\n",
      "            1.1399e-01, -6.9520e-02],\n",
      "          [ 1.7940e-01,  1.1081e+00,  2.9595e-01,  ..., -9.3686e-01,\n",
      "           -6.0198e-01,  1.2316e-01],\n",
      "          [ 5.6801e-01,  1.2887e+00,  7.6268e-01,  ..., -5.3883e-01,\n",
      "            8.0207e-01, -6.4096e-01],\n",
      "          ...,\n",
      "          [ 3.2395e-01, -8.6910e-01, -1.2533e+00,  ...,  4.3327e-01,\n",
      "           -5.0208e-02,  1.6928e-01],\n",
      "          [ 3.5543e-01, -9.3007e-01, -1.1551e+00,  ...,  8.6482e-01,\n",
      "           -2.8278e-01,  1.6601e-01],\n",
      "          [-3.8553e-02, -6.1234e-01, -7.1148e-01,  ..., -2.2919e-01,\n",
      "           -8.9462e-01,  1.2913e+00]],\n",
      "\n",
      "         [[-1.0240e-01,  8.5114e-01,  1.1398e+00,  ..., -6.8985e-01,\n",
      "           -3.3074e-01, -5.8245e-01],\n",
      "          [ 2.8520e-01,  9.5164e-01,  1.9384e+00,  ..., -2.8128e-01,\n",
      "           -5.9496e-01, -3.3918e-01],\n",
      "          [-4.6677e-02,  1.0173e+00,  1.6954e+00,  ..., -4.7169e-01,\n",
      "           -3.9436e-01, -3.4464e-01],\n",
      "          ...,\n",
      "          [-4.6012e-02, -7.2739e-01, -8.2055e-01,  ...,  3.2047e-02,\n",
      "           -1.3377e-01,  4.1145e-01],\n",
      "          [-1.9049e-01, -4.7534e-01, -7.2514e-01,  ..., -2.1277e-01,\n",
      "           -2.9610e-01,  3.6651e-01],\n",
      "          [-2.4270e-01, -3.3445e-01, -7.0966e-01,  ..., -1.8586e-01,\n",
      "           -1.5720e-01,  4.6546e-01]]]], device='cuda:0')\n",
      "5 None None\n",
      "torch.Size([1, 8, 925, 2048])\n",
      "tensor([[[[ 0.5244,  1.8721,  0.6143,  ..., -0.9419, -0.4883, -1.8272],\n",
      "          [ 0.5051,  3.2486,  1.8848,  ..., -0.2367,  0.7393, -1.2829],\n",
      "          [ 0.2638,  3.2657,  2.0418,  ...,  0.0207,  0.7029, -1.5195],\n",
      "          ...,\n",
      "          [ 0.9885, -1.2110, -1.1610,  ..., -1.1070, -0.4766, -1.4052],\n",
      "          [ 1.0403, -1.3374, -1.3384,  ..., -0.7547, -0.7343, -1.6351],\n",
      "          [ 0.9755, -1.1852, -1.3169,  ..., -1.3520, -0.6852, -1.4287]],\n",
      "\n",
      "         [[ 0.1171,  0.5199, -0.3760,  ..., -0.8269, -0.3298, -0.9556],\n",
      "          [ 0.2504,  0.8076, -0.3937,  ..., -0.8191, -0.2015, -0.5212],\n",
      "          [ 0.7632,  0.8735, -0.3086,  ..., -0.8285, -0.0178, -0.0621],\n",
      "          ...,\n",
      "          [-0.6823,  0.6225,  0.6321,  ...,  3.0017, -0.6021, -1.7760],\n",
      "          [-1.5873,  1.2132,  0.5204,  ..., -1.5695, -1.5665, -0.5651],\n",
      "          [-0.9977,  0.8786,  0.5768,  ...,  1.7739, -1.1600, -2.1065]],\n",
      "\n",
      "         [[ 0.1583,  2.6542,  1.2219,  ..., -0.9643, -0.9958, -2.2093],\n",
      "          [ 0.2749,  4.0883,  2.9732,  ..., -0.3651,  0.3046, -2.1806],\n",
      "          [ 0.4011,  3.8683,  2.8706,  ..., -0.7675, -0.3213, -1.8485],\n",
      "          ...,\n",
      "          [ 0.8048, -2.2161, -1.2137,  ..., -0.1473,  0.3556, -1.5552],\n",
      "          [ 0.8557, -1.3582, -0.8687,  ..., -0.1011, -0.1126, -1.8937],\n",
      "          [ 0.9428, -1.2280, -0.9646,  ..., -0.2565, -0.0784, -2.0344]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1409,  0.5452, -2.3071,  ...,  1.0499,  1.4547, -1.4422],\n",
      "          [ 1.7600,  0.2766, -2.0613,  ...,  2.4385,  1.4416, -1.3818],\n",
      "          [ 0.9318,  0.0611, -1.7670,  ...,  1.8131,  0.9772, -1.7055],\n",
      "          ...,\n",
      "          [-0.1856,  1.2957,  1.4098,  ..., -1.5759,  0.3535, -1.2459],\n",
      "          [ 0.2431,  1.6823,  1.3662,  ..., -1.9389,  0.4282, -0.8320],\n",
      "          [ 0.1741,  1.4913,  1.1322,  ..., -1.7967,  0.3515, -1.0028]],\n",
      "\n",
      "         [[ 1.8105,  2.6517,  1.6006,  ..., -1.3015,  0.2489, -1.2351],\n",
      "          [ 1.2455,  2.2054,  1.0354,  ..., -1.9106,  0.0220, -1.5798],\n",
      "          [ 2.0130,  2.2045,  1.3195,  ..., -2.1543,  1.2088, -2.0332],\n",
      "          ...,\n",
      "          [ 0.8008, -0.0542, -1.0230,  ..., -0.6252,  0.0982, -0.6408],\n",
      "          [ 1.0512, -0.1368, -1.0008,  ..., -0.2273, -0.1953, -0.6769],\n",
      "          [ 0.2636,  0.1235, -0.8421,  ..., -0.2527, -0.1537, -0.1211]],\n",
      "\n",
      "         [[ 0.4416,  1.9277,  1.2598,  ...,  1.0158,  0.6545, -1.5941],\n",
      "          [ 0.4499,  1.8662,  1.9155,  ...,  0.9847,  0.5724, -1.4549],\n",
      "          [ 0.1191,  2.3348,  1.9584,  ...,  0.6582,  0.9331, -1.5098],\n",
      "          ...,\n",
      "          [ 1.1476,  0.4234, -0.0798,  ..., -0.5628,  0.1077, -1.2007],\n",
      "          [ 1.1146,  0.6258, -0.0542,  ..., -0.9200, -0.2859, -1.2362],\n",
      "          [ 1.2390,  0.7390, -0.0752,  ..., -0.8602, -0.2693, -1.0987]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mps_idx_list: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ps_idx_list))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregated_tokens_tensor: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(aggregated_tokens_tensor\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m---> 63\u001b[0m track_list, vis_score, conf_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_vggt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregated_tokens_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mps_idx_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_points\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Draw bounding box\u001b[39;00m\n\u001b[1;32m     71\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(overlay, (x0, y0), (x1, y1), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/vggt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/vggt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/vggt/vggt/heads/track_head.py:102\u001b[0m, in \u001b[0;36mTrackHead.forward\u001b[0;34m(self, aggregated_tokens_list, images, patch_start_idx, query_points, iters)\u001b[0m\n\u001b[1;32m     99\u001b[0m     iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miters\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Perform tracking using the extracted features\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m coord_preds, vis_scores, conf_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmaps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coord_preds, vis_scores, conf_scores\n",
      "File \u001b[0;32m~/Documents/vggt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/vggt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/vggt/vggt/heads/track_modules/base_track_predictor.py:145\u001b[0m, in \u001b[0;36mBaseTrackerPredictor.forward\u001b[0;34m(self, query_points, fmaps, iters, return_feat, down_ratio, apply_sigmoid)\u001b[0m\n\u001b[1;32m    142\u001b[0m track_feats_ \u001b[38;5;241m=\u001b[39m track_feats\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m N, S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Concatenate them as the input for the transformers\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m transformer_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mflows_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcorrs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_feats_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# 2D positional embed\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# TODO: this can be much simplified\u001b[39;00m\n\u001b[1;32m    149\u001b[0m pos_embed \u001b[38;5;241m=\u001b[39m get_2d_sincos_pos_embed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_dim, grid_size\u001b[38;5;241m=\u001b[39m(HH, WW))\u001b[38;5;241m.\u001b[39mto(query_points\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "# Load image\n",
    "SCORE_THRESHOLD = 0.5\n",
    "# image_paths = [\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"]\n",
    "# # image_paths = [\"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"]\n",
    "# # image_paths = [\"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"]\n",
    "# # image_paths = [\"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"]\n",
    "# # image_paths = [\"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"]\n",
    "# # image_paths = [\"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"]\n",
    "# image_paths = []\n",
    "\n",
    "object_masks = []\n",
    "\n",
    "for image_path in image_names:\n",
    "    image = Image.open(image_path).convert(\"RGB\").resize((518, 350), resample=Image.BILINEAR) # <--- match VGGT\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Text prompt\n",
    "    # texts = [[\"car\", \"bicycle\", \"trees\", \"grass\", \"ground\", \"bench\", \"lego\", \"fruit\"]]  # You can modify this list\n",
    "    texts = [[\"truck\", \"bicycle\", \"ground\", \"bench\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]]\n",
    "\n",
    "    # Prepare input for OWL-ViT\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Detect with OWL-ViT\n",
    "    # print(\"Begin detect\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # print(\"End detect\")\n",
    "\n",
    "    # Get boxes and scores\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=SCORE_THRESHOLD)[0]\n",
    "\n",
    "    # Run SAM\n",
    "    predictor.set_image(image_np)\n",
    "\n",
    "    # Process each box from OWL-ViT\n",
    "    for i, (box, score, label) in enumerate(zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"])):\n",
    "\n",
    "        # Modify the class as needed\n",
    "        if texts[0][label] == \"bicycle\":\n",
    "            box = box.cpu().numpy().astype(int)\n",
    "            x0, y0, x1, y1 = box\n",
    "            print(f\"{texts[0][label]}: {score:.2f} at box {box}\")\n",
    "\n",
    "            # SAM expects box in XYXY format\n",
    "            input_box = np.array([x0, y0, x1, y1])\n",
    "            masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)\n",
    "\n",
    "            # Overlay mask\n",
    "            mask = masks[0]\n",
    "            overlay = image_np.copy()\n",
    "            overlay[mask] = (255, 0, 0)  # Red mask\n",
    "\n",
    "            # Convert mask to query points\n",
    "            query_points = mask_to_query_points(mask, max_points=100).to(\"cuda\")\n",
    "\n",
    "            # Track\n",
    "            print(\"query_points.shape: {}\".format(query_points.shape))\n",
    "            print(\"all_images.shape: {}\".format(all_images.shape))\n",
    "            print(\"ps_idx_list: {}\".format(ps_idx_list))\n",
    "            print(\"aggregated_tokens_tensor: {}\".format(aggregated_tokens_tensor.shape))\n",
    "            track_list, vis_score, conf_score = model_vggt.track_head(\n",
    "                aggregated_tokens_tensor, \n",
    "                all_images[None], \n",
    "                ps_idx_list[0], \n",
    "                query_points=query_points[None]\n",
    "            )\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(overlay, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "            cv2.putText(overlay, f\"{texts[0][label]} {score:.2f}\", (x0, y0 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "            # Show\n",
    "            plt.imshow(overlay)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            resized_mask = cv2.resize(mask.astype(np.uint8), (depth_conf_maps[0].shape[1], depth_conf_maps[0].shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            resized_mask = resized_mask[None, ...].astype(np.float32)\n",
    "            \n",
    "            # Visualize\n",
    "            track_list = torch.stack(track_list)\n",
    "            query_points = query_points.detach().cpu().numpy()\n",
    "            visualize_tracks(all_images, track_list, query_points)\n",
    "\n",
    "            # break\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    object_masks.append(resized_mask)\n",
    "\n",
    "object_masks = np.concatenate((object_masks), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1da09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mask.view(depth_conf_maps[0].shape[0], depth_conf_maps[0].shape[1]).shape, depth_conf_maps[0].shape)\n",
    "print(mask.shape, resized_mask.shape, depth_conf_maps[0].shape, object_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08dcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = all_intrinsics.shape[0]\n",
    "MASKING_MODE = \"object\"\n",
    "\n",
    "for i in range(N):\n",
    "    # filter out pixels\n",
    "    if MASKING_MODE == \"depth\":\n",
    "        conf_mask = depth_conf_maps[i]\n",
    "    elif MASKING_MODE == \"object\":\n",
    "        conf_mask = object_masks[i]\n",
    "    else:\n",
    "        assert MASKING_MODE == \"depth\" or MASKING_MODE == \"object\"\n",
    "\n",
    "    conf_mask /= conf_mask.max()\n",
    "    conf_mask[conf_mask<0.5] = 0.0\n",
    "    conf_mask[conf_mask>0.0] = 1.0\n",
    "    all_world_points[i, :] *= conf_mask[..., None]\n",
    "    depth_maps[i, :] *= conf_mask[..., None]\n",
    "    all_images[i, :] *= conf_mask[None, ...].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9eed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da43e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
