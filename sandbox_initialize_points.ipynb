{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e174d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGT(\n",
       "  (aggregator): Aggregator(\n",
       "    (patch_embed): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (rope): RotaryPositionEmbedding2D()\n",
       "    (frame_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (global_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (camera_head): CameraHead(\n",
       "    (trunk): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (token_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (trunk_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_pose): Linear(in_features=9, out_features=2048, bias=True)\n",
       "    (poseLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    )\n",
       "    (adaln_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "    (pose_branch): Mlp(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (point_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (track_head): TrackHead(\n",
       "    (feature_extractor): DPTHead(\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (projects): ModuleList(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (resize_layers): ModuleList(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (2): Identity()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (refinenet1): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (output_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (tracker): BaseTrackerPredictor(\n",
       "      (corr_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=567, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (updateformer): EfficientUpdateFormer(\n",
       "        (input_norm): LayerNorm((388,), eps=1e-05, elementwise_affine=True)\n",
       "        (input_transform): Linear(in_features=388, out_features=384, bias=True)\n",
       "        (output_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (flow_head): Linear(in_features=384, out_features=130, bias=True)\n",
       "        (time_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_point2virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual2point_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fmap_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffeat_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (ffeat_updater): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (vis_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (conf_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT()\n",
    "_URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0cb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE=\"bicycle\"\n",
    "SKIP=5\n",
    "\n",
    "if SCENE==\"banana\": \n",
    "    # Load and preprocess example images (replace with your own image paths)\n",
    "    image_names = [\n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00001.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00002.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00003.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00004.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00005.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00006.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00007.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00008.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00009.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00010.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00011.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00012.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00013.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00014.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00015.JPG\", \n",
    "        \"/home/skhalid/Documents/data/banana/input/frame_00016.JPG\"\n",
    "    ]\n",
    "    ### BANANA\n",
    "    width = 3008\n",
    "    height = 2000\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/banana\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"frame_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"lego\": \n",
    "    ### LEGO\n",
    "    image_names = [\"/home/skhalid/Documents/data/nerf_synthetic/lego/train/r_\"+str(v)+\".png\" for v in range(0, 99, SKIP)]\n",
    "    width = 800\n",
    "    height = 800\n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/nerf_synthetic/lego/\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"r_\"\n",
    "    START_ID = 0\n",
    "    N = 200_000\n",
    "\n",
    "elif SCENE==\"bicycle\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/360_v2/bicycle/images_4/_DSC\"\n",
    "    image_names = [BASE+str(v)+\".JPG\" for v in range(8679, 8873, SKIP)]\n",
    "    width = 1236\n",
    "    height = 821    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/360_v2/bicycle\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"_DSC\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n",
    "\n",
    "elif SCENE==\"truck\": \n",
    "    ### BICYCLE\n",
    "    BASE=\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/\"\n",
    "    image_names = [BASE+str(v).zfill(6)+\".jpg\" for v in range(1, 252, SKIP)]\n",
    "    width = 1957\n",
    "    height = 1091    \n",
    "    BASE_PATH = \"/home/skhalid/Documents/data/tandt_db/tandt/truck\"\n",
    "    INTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/cameras.bin\"\n",
    "    EXTRINSICS_BINARY_PATH = BASE_PATH+\"/sparse/0/images.bin\"\n",
    "    PTS_PATH = BASE_PATH+\"/sparse/0/points3D.ply\"\n",
    "    PREFIX = \"\"\n",
    "    START_ID = 0\n",
    "    N = 1_000_000\n",
    "    # test_cases = [\"8679.JPG\",\n",
    "    #               \"8687.JPG\",\n",
    "    #               \"8695.JPG\",\n",
    "    #               \"8703.JPG\",\n",
    "    #               \"8711.JPG\",\n",
    "    #               \"8719.JPG\",\n",
    "    #               \"8727.JPG\",\n",
    "    #               \"8735.JPG\",\n",
    "    #               \"8744.JPG\",\n",
    "    #               \"8752.JPG\",\n",
    "    #               \"8760.JPG\",\n",
    "    #               \"8768.JPG\",\n",
    "    #               \"8776.JPG\",\n",
    "    #               \"8784.JPG\",\n",
    "    #               \"8792.JPG\",\n",
    "    #               \"8800.JPG\",\n",
    "    #               \"8808.JPG\",\n",
    "    #               \"8816.JPG\",\n",
    "    #               \"8824.JPG\",\n",
    "    #               \"8832.JPG\",\n",
    "    #               \"8840.JPG\",\n",
    "    #               \"8848.JPG\",\n",
    "    #               \"8856.JPG\",\n",
    "    #               \"8864.JPG\",\n",
    "    #               \"8872.JPG\"]\n",
    "    # for test_case in test_cases:\n",
    "    #     image_names.append(BASE+str(test_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b124f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_batched_camera_inference(model, image_names, batch_size=8, device='cuda', dtype=torch.float16):\n",
    "    from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "    from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "    # from vggt.utils.io import load_and_preprocess_images\n",
    "\n",
    "    all_extrinsics = []\n",
    "    all_intrinsics = []\n",
    "    all_world_points = []\n",
    "    depth_maps = []\n",
    "    depth_conf_maps = []\n",
    "    batch_tensors = []\n",
    "\n",
    "    # Batch the rest of the images\n",
    "    print(f\"Processing the rest of {len(image_names)} images in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i + batch_size]\n",
    "        batch_tensor = load_and_preprocess_images(batch_names).to(device)\n",
    "\n",
    "        if i==0:\n",
    "            first_image = batch_tensor[0]\n",
    "            print(\"first_image.shape: {}\".format(batch_tensor.shape))\n",
    "        else:\n",
    "            # Add the first reference image to this batch as well\n",
    "            batch_tensor = torch.cat((first_image[None], batch_tensor), dim=0)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n",
    "            batch_tensor = batch_tensor[None]  # Add batch dim\n",
    "            agg_tokens, ps_idx = model.aggregator(batch_tensor)\n",
    "\n",
    "            pose_enc = model.camera_head(agg_tokens)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, batch_tensor.shape[-2:])\n",
    "\n",
    "            depth_map, depth_conf_map = model.depth_head(agg_tokens, batch_tensor, ps_idx)\n",
    "            \n",
    "            point_map_unproj = unproject_depth_map_to_point_map(depth_map.squeeze(0), extrinsic.squeeze(0), intrinsic.squeeze(0))\n",
    "    \n",
    "            # if i==0:    \n",
    "            #     print(\"batch: {} | point_map_unproj.shape: {}\".format(i, point_map_unproj.shape))\n",
    "            # else:\n",
    "            #     print(\"batch: {} | point_map_unproj[1:, ...].shape: {}\".format(i, point_map_unproj[1:, ...].shape))\n",
    "\n",
    "            if i==0:\n",
    "                all_extrinsics.append(extrinsic[0, ...])\n",
    "                all_intrinsics.append(intrinsic[0, ...])\n",
    "                all_world_points.append(point_map_unproj)\n",
    "                depth_maps.append(depth_map[0, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, ...])\n",
    "                batch_tensors.append(batch_tensor[0, ...])\n",
    "            else:\n",
    "                all_extrinsics.append(extrinsic[0, 1:])\n",
    "                all_intrinsics.append(intrinsic[0, 1:])\n",
    "                all_world_points.append(point_map_unproj[1:, ...])\n",
    "                depth_maps.append(depth_map[0, 1:, ...])\n",
    "                depth_conf_maps.append(depth_conf_map[0, 1:, ...])\n",
    "                batch_tensors.append(batch_tensor[0, 1:, ...])\n",
    "\n",
    "            print(\"extrinsic: {}\".format(extrinsic.shape))\n",
    "            print(\"intrinsic: {}\".format(intrinsic.shape))\n",
    "            print(\"point_map_unproj: {}\".format(point_map_unproj.shape))\n",
    "            print(\"depth_map: {}\".format(depth_map.shape))\n",
    "            print(\"depth_conf_map: {}\".format(depth_conf_map.shape))\n",
    "            print(\"batch_tensor: {}\".format(batch_tensor.shape))\n",
    "\n",
    "    # Stack everything\n",
    "    batch_tensors = torch.cat(batch_tensors)  # [N, 4, 4]\n",
    "    all_extrinsics = torch.cat(all_extrinsics)  # [N, 4, 4]\n",
    "    all_intrinsics = torch.cat(all_intrinsics)  # [N, 3, 3]\n",
    "    all_world_points = np.concatenate(all_world_points)  # [N, H, W, 3]\n",
    "    depth_maps = torch.cat(depth_maps, dim=0)  # [N, H, W, 3]\n",
    "    depth_conf_maps = torch.cat(depth_conf_maps, dim=0)  # [N, H, W, 3]\n",
    "\n",
    "    return {\n",
    "        \"all_extrinsics\": all_extrinsics, \n",
    "        \"all_intrinsics\": all_intrinsics, \n",
    "        \"all_world_points\": all_world_points,\n",
    "        \"depth_maps\": depth_maps,\n",
    "        \"depth_conf_maps\": depth_conf_maps,\n",
    "        \"all_images\": batch_tensors\n",
    "    }\n",
    "\n",
    "    # # Predict Tracks\n",
    "    # # choose your own points to track, with shape (N, 2) for one scene\n",
    "    # query_points = torch.FloatTensor([[100.0, 200.0], \n",
    "    #                                     [60.72, 259.94]]).to(device)\n",
    "    # track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4d6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=3\n",
    "\n",
    "# res = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "# all_extrinsics = res[\"all_extrinsics\"]\n",
    "# all_intrinsics = res[\"all_intrinsics\"]\n",
    "# all_world_points = res[\"all_world_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_world_points.shape)\n",
    "# print(all_extrinsics.shape)\n",
    "# print(all_intrinsics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38ddc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def pose_tensor_to_matrix(translation, quaternion):\n",
    "    \"\"\"Convert translation + quaternion to 4x4 pose matrix.\"\"\"\n",
    "    r = R.from_quat(quaternion.cpu().numpy())\n",
    "    R_mat = r.as_matrix()  # [3, 3]\n",
    "\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = R_mat\n",
    "    pose_matrix[:3, 3] = translation.cpu().numpy()\n",
    "\n",
    "    return pose_matrix.tolist()\n",
    "\n",
    "def export_poses_as_json(pred_pose_tensor, output_path, image_folder=\"images\"):\n",
    "    frames = []\n",
    "\n",
    "    # If batched, flatten to a list\n",
    "    poses = pred_pose_tensor.view(-1, pred_pose_tensor.shape[-1])  # [B * N, 10]\n",
    "\n",
    "    for i, pose in enumerate(poses):\n",
    "        translation = pose[0:3]\n",
    "        quaternion = pose[3:7]\n",
    "        fx = pose[7].item()\n",
    "        fy = pose[8].item()\n",
    "        cx = 0.5  # assuming normalized cx/cy; adjust as needed\n",
    "        cy = 0.5\n",
    "\n",
    "        transform_matrix = pose_tensor_to_matrix(translation, quaternion)\n",
    "\n",
    "        frame = {\n",
    "            \"file_path\": f\"{image_folder}/{i:06d}.png\",\n",
    "            \"transform_matrix\": transform_matrix,\n",
    "            \"intrinsics\": {\n",
    "                \"fx\": fx,\n",
    "                \"fy\": fy,\n",
    "                \"cx\": cx,\n",
    "                \"cy\": cy\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(frame)\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    data = {\n",
    "        \"frames\": frames\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Saved poses to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc464ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_intrinsics_binary_from_poses(intrinsics_tensor, width, height, dims, output_path, camera_model=\"PINHOLE\", start_id=1):\n",
    "    \"\"\"\n",
    "    Write camera intrinsics from a tensor of 3x3 matrices.\n",
    "\n",
    "    Args:\n",
    "        intrinsics_tensor: Tensor of shape [B, N, 3, 3].\n",
    "        width: Image width.\n",
    "        height: Image height.\n",
    "        output_path: Path to cameras.bin.\n",
    "        camera_model: Camera model name.\n",
    "        start_id: Starting camera ID.\n",
    "    \"\"\"\n",
    "    CAMERA_MODEL_IDS = {\n",
    "        \"SIMPLE_PINHOLE\": 0,\n",
    "        \"PINHOLE\": 1,\n",
    "        \"SIMPLE_RADIAL\": 2,\n",
    "        \"RADIAL\": 3,\n",
    "        \"OPENCV\": 4,\n",
    "    }\n",
    "    model_id = CAMERA_MODEL_IDS.get(camera_model.upper(), 1)\n",
    "\n",
    "    intrinsics_tensor = intrinsics_tensor.view(-1, 3, 3)  # [num_cameras, 3, 3]\n",
    "    num_cameras = intrinsics_tensor.shape[0]\n",
    "    w, h = dims\n",
    "\n",
    "\n",
    "    SCALE = width / w # <---------------------------------------------------------------------------------------FIXME\n",
    "    # SCALE = 2.0 # <---------------------------------------------------------------------------------------FIXME\n",
    "    print(\"width: {} | w: {} | scale: {}\".format(width, w, SCALE))\n",
    "\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack(\"Q\", num_cameras))  # Number of cameras\n",
    "\n",
    "        for i in range(num_cameras):\n",
    "            intrinsics = intrinsics_tensor[i].cpu().numpy()\n",
    "            fx = intrinsics[0, 0] * SCALE\n",
    "            fy = intrinsics[1, 1] * SCALE\n",
    "            cx = intrinsics[0, 2] * SCALE\n",
    "            cy = intrinsics[1, 2] * SCALE\n",
    "\n",
    "            params = [fx, fy, cx, cy]\n",
    "            camera_id = start_id + i\n",
    "\n",
    "            # Write: camera_id, model_id, width, height\n",
    "            f.write(struct.pack(\"iiQQ\", camera_id, model_id, width, height))\n",
    "            # Write parameters\n",
    "            f.write(struct.pack(\"d\" * len(params), *params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f55f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_poses_as_json(predictions[\"pose_enc\"][0], \"output_path\", image_folder=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe8d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_colmap_format(fns, extrinsics_tensor, image_name_prefix=\"_DSC\", start_id=8679):\n",
    "    \"\"\"\n",
    "    Convert extrinsics matrices into COLMAP-format dictionaries.\n",
    "\n",
    "    Args:\n",
    "        extrinsics_tensor: Tensor of shape [B, N, 3, 4] containing [R|t].\n",
    "        image_name_prefix: Prefix for image names.\n",
    "        start_id: Starting ID for images.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of COLMAP-style extrinsics.\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    extrinsics_tensor = extrinsics_tensor.view(-1, 3, 4)  # [num_poses, 3, 4]\n",
    "    translation_scale_factor = 5.0  # Current (probably too small)\n",
    "    print(extrinsics_tensor)\n",
    "\n",
    "    for idx, extrinsic in enumerate(extrinsics_tensor, start=start_id):\n",
    "        # Extract rotation matrix and translation\n",
    "        R = extrinsic[:, :3].cpu().numpy()\n",
    "        t = extrinsic[:, 3].cpu().numpy()\n",
    "\n",
    "        # Convert rotation matrix to quaternion (COLMAP uses [w, x, y, z] format)\n",
    "        from scipy.spatial.transform import Rotation\n",
    "        qvec = Rotation.from_matrix(R).as_quat()  # [x, y, z, w]\n",
    "        qvec_colmap = np.array([qvec[3], qvec[0], qvec[1], qvec[2]])  # Reorder to [w, x, y, z]\n",
    "        qvec_colmap = qvec_colmap / np.linalg.norm(qvec_colmap)  # Ensure it's normalized\n",
    "\n",
    "        image_name = f\"{image_name_prefix}{idx:05d}.JPG\"\n",
    "        image_name = fns[idx]\n",
    "\n",
    "        images[idx] = {\n",
    "            \"id\": idx,\n",
    "            \"qvec\": qvec_colmap,\n",
    "            \"tvec\": t,\n",
    "            \"camera_id\": 1,\n",
    "            \"name\": image_name,\n",
    "            \"xys\": np.zeros((0, 2)),\n",
    "            \"point3D_ids\": np.array([])\n",
    "        }\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53974676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def write_extrinsics_binary(images, output_path):\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        # Write number of registered images (uint64)\n",
    "        f.write(struct.pack(\"Q\", len(images)))\n",
    "\n",
    "        for image_id, img in images.items():\n",
    "            # Write: image_id (uint32), qvec (4 doubles), tvec (3 doubles), camera_id (uint32)\n",
    "            f.write(struct.pack(\"i\", img[\"id\"]))  # IMAGE_ID\n",
    "            f.write(struct.pack(\"dddd\", *img[\"qvec\"]))  # qw, qx, qy, qz\n",
    "            f.write(struct.pack(\"ddd\", *img[\"tvec\"]))  # tx, ty, tz\n",
    "            f.write(struct.pack(\"i\", img[\"camera_id\"]))  # CAMERA_ID\n",
    "\n",
    "            # Write the image name (null-terminated string)\n",
    "            f.write(img[\"name\"].encode(\"utf-8\") + b'\\x00')\n",
    "\n",
    "            # No 2D points\n",
    "            num_points2D = 0\n",
    "            f.write(struct.pack(\"Q\", num_points2D))\n",
    "\n",
    "            # (Skip point data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f854fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, h, w, c = all_world_points.shape\n",
    "# images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "# write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "# write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1549021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def save_point_cloud_ply(filepath, points, colors=None, normals=None):\n",
    "    \"\"\"\n",
    "    Save a point cloud to a PLY file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Output path.\n",
    "        points: (N, 3) numpy array of XYZ coordinates.\n",
    "        colors: (N, 3) numpy array of RGB values in [0, 1] or [0, 255]. Optional.\n",
    "        normals: (N, 3) numpy array of normals. Optional.\n",
    "    \"\"\"\n",
    "\n",
    "    num_points = points.shape[0]\n",
    "\n",
    "    # Default colors  white\n",
    "    if colors is None:\n",
    "        colors = np.ones_like(points) * 255\n",
    "    else:\n",
    "        # If in [0, 1], scale to [0, 255]\n",
    "        if colors.max() <= 1.0:\n",
    "            colors = (colors * 255).astype(np.uint8)\n",
    "\n",
    "    # Default normals  zero\n",
    "    if normals is None:\n",
    "        normals = np.zeros_like(points)\n",
    "\n",
    "    # Define PLY vertex structure\n",
    "    vertex_data = np.array(\n",
    "        [\n",
    "            (\n",
    "                points[i, 0], points[i, 1], points[i, 2],\n",
    "                normals[i, 0], normals[i, 1], normals[i, 2],\n",
    "                colors[i, 0], colors[i, 1], colors[i, 2],\n",
    "            )\n",
    "            for i in range(num_points)\n",
    "        ],\n",
    "        dtype=[\n",
    "            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "            ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),\n",
    "            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create PlyElement and write to file\n",
    "    ply_element = PlyElement.describe(vertex_data, 'vertex')\n",
    "    PlyData([ply_element]).write(filepath)\n",
    "\n",
    "    print(f\"Saved point cloud with {num_points} points to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee4e427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['images']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example image tensor: replace this with your actual variable\n",
    "# images = predictions['depth']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aeade52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = predictions['depth_conf']  # shape: [1, 16, 350, 518, 1]\n",
    "\n",
    "# # Remove batch dimension and channel dimension\n",
    "# images = images.squeeze(0).squeeze(-1)  # Now shape is [16, 350, 518]\n",
    "\n",
    "# # Loop and visualize\n",
    "# for i in range(images.shape[0]):\n",
    "#     img = images[i].cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     if img.ndim == 2:\n",
    "#         plt.imshow(img, cmap='gray')  # Grayscale\n",
    "#     elif img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "#         # Permute if channels first\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image shape: {img.shape}\")\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image {i}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f172f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the rest of 39 images in batches of 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_3265638/2588084714.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_image.shape: torch.Size([7, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 1/6 [00:00<00:02,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 7, 3, 4])\n",
      "intrinsic: torch.Size([1, 7, 3, 3])\n",
      "point_map_unproj: (7, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 7, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 7, 350, 518])\n",
      "batch_tensor: torch.Size([1, 7, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 2/6 [00:01<00:02,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 8, 3, 4])\n",
      "intrinsic: torch.Size([1, 8, 3, 3])\n",
      "point_map_unproj: (8, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 8, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 8, 350, 518])\n",
      "batch_tensor: torch.Size([1, 8, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 3/6 [00:01<00:01,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 8, 3, 4])\n",
      "intrinsic: torch.Size([1, 8, 3, 3])\n",
      "point_map_unproj: (8, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 8, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 8, 350, 518])\n",
      "batch_tensor: torch.Size([1, 8, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 4/6 [00:01<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 8, 3, 4])\n",
      "intrinsic: torch.Size([1, 8, 3, 3])\n",
      "point_map_unproj: (8, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 8, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 8, 350, 518])\n",
      "batch_tensor: torch.Size([1, 8, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 5/6 [00:02<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 8, 3, 4])\n",
      "intrinsic: torch.Size([1, 8, 3, 3])\n",
      "point_map_unproj: (8, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 8, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 8, 350, 518])\n",
      "batch_tensor: torch.Size([1, 8, 3, 350, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrinsic: torch.Size([1, 5, 3, 4])\n",
      "intrinsic: torch.Size([1, 5, 3, 3])\n",
      "point_map_unproj: (5, 350, 518, 3)\n",
      "depth_map: torch.Size([1, 5, 350, 518, 1])\n",
      "depth_conf_map: torch.Size([1, 5, 350, 518])\n",
      "batch_tensor: torch.Size([1, 5, 3, 350, 518])\n",
      "tensor([[[ 1.0000e+00,  1.1086e-05,  3.5524e-05,  3.2127e-05],\n",
      "         [-1.1086e-05,  1.0000e+00,  3.4809e-05, -1.0550e-05],\n",
      "         [-3.5524e-05, -3.4809e-05,  1.0000e+00,  2.5570e-05]],\n",
      "\n",
      "        [[ 8.8008e-01,  4.9203e-02, -4.7210e-01,  3.5986e-01],\n",
      "         [ 1.8019e-01,  8.8558e-01,  4.2810e-01, -1.7319e-02],\n",
      "         [ 4.3934e-01, -4.6183e-01,  7.7055e-01,  5.8594e-02]],\n",
      "\n",
      "        [[ 2.7906e-01,  7.3267e-02, -9.5735e-01,  7.6514e-01],\n",
      "         [-9.4941e-03,  9.9724e-01,  7.3572e-02, -8.7128e-03],\n",
      "         [ 9.6028e-01, -1.1478e-02,  2.7906e-01,  4.4922e-01]],\n",
      "\n",
      "        [[-2.6098e-01,  7.9911e-02, -9.6235e-01,  7.4902e-01],\n",
      "         [ 3.7389e-01,  9.2718e-01, -2.4193e-02,  3.0176e-01],\n",
      "         [ 8.9002e-01, -3.6607e-01, -2.7173e-01,  8.3887e-01]],\n",
      "\n",
      "        [[-9.6757e-01,  5.9381e-02, -2.4680e-01,  1.6089e-01],\n",
      "         [ 1.9651e-02,  9.8686e-01,  1.6038e-01, -7.4219e-02],\n",
      "         [ 2.5315e-01,  1.5038e-01, -9.5586e-01,  1.3555e+00]],\n",
      "\n",
      "        [[-9.7881e-01, -3.1072e-02,  2.0269e-01, -2.7759e-01],\n",
      "         [-1.0638e-01,  9.2203e-01, -3.7225e-01,  5.0195e-01],\n",
      "         [-1.7521e-01, -3.8600e-01, -9.0617e-01,  8.5254e-01]],\n",
      "\n",
      "        [[-4.7228e-01, -6.6894e-02,  8.7858e-01, -5.6152e-01],\n",
      "         [ 7.2016e-02,  9.9084e-01,  1.1427e-01, -4.0863e-02],\n",
      "         [-8.7858e-01,  1.1732e-01, -4.6350e-01,  9.6436e-01]],\n",
      "\n",
      "        [[ 9.9839e-02, -1.1020e-01,  9.8901e-01, -7.4219e-01],\n",
      "         [-3.2414e-01,  9.3605e-01,  1.3693e-01,  1.4648e-01],\n",
      "         [-9.4117e-01, -3.3439e-01,  5.7370e-02,  6.6797e-01]],\n",
      "\n",
      "        [[ 7.6700e-01, -3.8466e-02,  6.4037e-01, -5.5762e-01],\n",
      "         [ 6.5393e-02,  9.9769e-01, -1.8378e-02,  4.2084e-02],\n",
      "         [-6.3842e-01,  5.5990e-02,  7.6786e-01,  1.6455e-01]],\n",
      "\n",
      "        [[ 9.7921e-01, -1.3139e-02,  2.0253e-01, -2.3950e-01],\n",
      "         [-6.9241e-02,  9.1634e-01,  3.9430e-01, -7.9346e-02],\n",
      "         [-1.9079e-01, -4.0017e-01,  8.9635e-01,  8.8196e-03]],\n",
      "\n",
      "        [[ 9.1954e-01,  4.9328e-02, -3.8999e-01,  2.8247e-01],\n",
      "         [-1.0977e-01,  9.8486e-01, -1.3419e-01,  7.8491e-02],\n",
      "         [ 3.7729e-01,  1.6618e-01,  9.1099e-01, -2.9816e-02]],\n",
      "\n",
      "        [[ 7.0083e-01,  3.5139e-02, -7.1253e-01,  5.6104e-01],\n",
      "         [ 1.5605e-01,  9.6703e-01,  2.0119e-01,  6.5674e-02],\n",
      "         [ 6.9594e-01, -2.5232e-01,  6.7228e-01,  1.2915e-01]],\n",
      "\n",
      "        [[ 2.1014e-01,  4.2497e-02, -9.7645e-01,  7.8174e-01],\n",
      "         [-2.5230e-01,  9.6758e-01, -1.2090e-02, -3.1924e-04],\n",
      "         [ 9.4421e-01,  2.4900e-01,  2.1405e-01,  4.9585e-01]],\n",
      "\n",
      "        [[-3.9729e-01,  6.4413e-02, -9.1572e-01,  6.7139e-01],\n",
      "         [-2.2575e-01,  9.6003e-01,  1.6549e-01, -1.2585e-01],\n",
      "         [ 8.8932e-01,  2.7232e-01, -3.6698e-01,  9.7266e-01]],\n",
      "\n",
      "        [[-7.8992e-01,  1.0017e-01, -6.0493e-01,  4.8975e-01],\n",
      "         [ 9.1641e-02,  9.9477e-01,  4.5089e-02,  1.4893e-01],\n",
      "         [ 6.0590e-01, -1.9803e-02, -7.9479e-01,  1.4424e+00]],\n",
      "\n",
      "        [[-9.9929e-01,  1.3032e-02,  4.0212e-02, -3.7003e-03],\n",
      "         [ 2.7855e-02,  9.1860e-01,  3.9398e-01, -3.4766e-01],\n",
      "         [-3.1821e-02,  3.9496e-01, -9.1795e-01,  1.4512e+00]],\n",
      "\n",
      "        [[-8.5343e-01, -2.8807e-02,  5.2048e-01, -3.8550e-01],\n",
      "         [-4.8094e-02,  9.9857e-01, -2.3528e-02,  1.9189e-01],\n",
      "         [-5.1951e-01, -4.5133e-02, -8.5343e-01,  1.3740e+00]],\n",
      "\n",
      "        [[-2.3408e-01, -1.1112e-01,  9.6608e-01, -7.5391e-01],\n",
      "         [ 2.5165e-01,  9.5265e-01,  1.7074e-01, -1.8811e-01],\n",
      "         [-9.3972e-01,  2.8314e-01, -1.9503e-01,  9.8975e-01]],\n",
      "\n",
      "        [[ 2.3018e-01, -6.8533e-02,  9.7084e-01, -7.7100e-01],\n",
      "         [-1.8659e-01,  9.7591e-01,  1.1322e-01,  1.2659e-01],\n",
      "         [-9.5519e-01, -2.0713e-01,  2.1208e-01,  6.2451e-01]],\n",
      "\n",
      "        [[ 8.9983e-01,  8.3065e-03,  4.3609e-01, -4.2261e-01],\n",
      "         [ 8.8440e-02,  9.7560e-01, -2.0107e-01,  1.7041e-01],\n",
      "         [-4.2730e-01,  2.1939e-01,  8.7711e-01,  2.9236e-02]],\n",
      "\n",
      "        [[ 9.9982e-01,  1.3865e-02,  1.3155e-02,  3.6883e-04],\n",
      "         [-1.7382e-02,  9.4564e-01,  3.2500e-01, -3.3600e-02],\n",
      "         [-7.9357e-03, -3.2500e-01,  9.4570e-01, -1.1725e-01]],\n",
      "\n",
      "        [[ 7.0376e-01,  4.0785e-02, -7.0922e-01,  6.8164e-01],\n",
      "         [-2.6840e-01,  9.3962e-01, -2.1223e-01,  2.1533e-01],\n",
      "         [ 6.5744e-01,  3.3971e-01,  6.7226e-01,  1.7212e-01]],\n",
      "\n",
      "        [[ 3.2866e-01,  9.8137e-02, -9.3938e-01,  9.0625e-01],\n",
      "         [ 2.8379e-03,  9.9448e-01,  1.0491e-01,  1.1572e-01],\n",
      "         [ 9.4426e-01, -3.7168e-02,  3.2671e-01,  4.1089e-01]],\n",
      "\n",
      "        [[-5.3221e-01,  4.4922e-02, -8.4570e-01,  7.4268e-01],\n",
      "         [-3.8134e-01,  8.7891e-01,  2.8637e-01, -2.5000e-01],\n",
      "         [ 7.5585e-01,  4.7485e-01, -4.5018e-01,  1.1709e+00]],\n",
      "\n",
      "        [[-9.9313e-01,  3.6272e-02, -1.1413e-01,  1.1066e-01],\n",
      "         [ 1.5800e-02,  9.8435e-01,  1.7562e-01,  8.9111e-03],\n",
      "         [ 1.1877e-01,  1.7244e-01, -9.7750e-01,  1.5586e+00]],\n",
      "\n",
      "        [[-3.8235e-01, -1.1329e-01,  9.1701e-01, -7.3486e-01],\n",
      "         [ 3.8290e-01,  8.8379e-01,  2.6852e-01, -2.7002e-01],\n",
      "         [-8.4092e-01,  4.5387e-01, -2.9455e-01,  1.1279e+00]],\n",
      "\n",
      "        [[ 4.1443e-01, -4.4572e-02,  9.0866e-01, -8.2959e-01],\n",
      "         [-5.1018e-03,  9.9867e-01,  5.1323e-02,  1.1438e-01],\n",
      "         [-9.0964e-01, -2.5906e-02,  4.1394e-01,  4.8877e-01]],\n",
      "\n",
      "        [[ 9.6768e-01,  2.6855e-02,  2.5073e-01, -2.4146e-01],\n",
      "         [ 6.8114e-02,  9.2951e-01, -3.6230e-01,  3.0518e-01],\n",
      "         [-2.4291e-01,  3.6767e-01,  8.9765e-01,  2.9755e-02]],\n",
      "\n",
      "        [[ 9.4216e-01,  5.8696e-02, -3.3014e-01,  2.1924e-01],\n",
      "         [-4.9356e-02,  9.9811e-01,  3.6628e-02,  2.3938e-01],\n",
      "         [ 3.3160e-01, -1.8207e-02,  9.4326e-01, -1.0828e-01]],\n",
      "\n",
      "        [[ 6.2246e-01,  4.1637e-02, -7.8145e-01,  6.3232e-01],\n",
      "         [-4.2296e-01,  8.5787e-01, -2.9134e-01,  2.1960e-01],\n",
      "         [ 6.5837e-01,  5.1185e-01,  5.5164e-01,  2.5635e-01]],\n",
      "\n",
      "        [[ 1.4838e-01,  6.6266e-02, -9.8647e-01,  8.0322e-01],\n",
      "         [-7.0785e-02,  9.9590e-01,  5.6219e-02,  2.2864e-01],\n",
      "         [ 9.8647e-01,  6.1532e-02,  1.5229e-01,  4.9390e-01]],\n",
      "\n",
      "        [[-6.1341e-01,  4.8647e-02, -7.8861e-01,  6.2207e-01],\n",
      "         [-3.7426e-01,  8.6103e-01,  3.4444e-01, -2.2351e-01],\n",
      "         [ 6.9572e-01,  5.0651e-01, -5.0976e-01,  1.1377e+00]],\n",
      "\n",
      "        [[-9.7787e-01, -9.1308e-03, -2.0741e-01,  1.2128e-01],\n",
      "         [-5.4143e-02,  9.7568e-01,  2.1242e-01,  7.9651e-02],\n",
      "         [ 2.0033e-01,  2.1902e-01, -9.5539e-01,  1.3701e+00]],\n",
      "\n",
      "        [[-6.9714e-01, -9.9114e-02,  7.1040e-01, -4.5435e-01],\n",
      "         [ 3.5642e-01,  8.1166e-01,  4.6261e-01, -3.3130e-01],\n",
      "         [-6.2251e-01,  5.7564e-01, -5.3016e-01,  1.0918e+00]],\n",
      "\n",
      "        [[ 3.4247e-01, -9.8569e-02,  9.3472e-01, -8.6377e-01],\n",
      "         [ 8.5614e-02,  9.9362e-01,  7.3392e-02,  1.7859e-01],\n",
      "         [-9.3570e-01,  5.4815e-02,  3.4882e-01,  4.3628e-01]],\n",
      "\n",
      "        [[ 8.7069e-01, -3.1777e-03,  4.9181e-01, -5.2539e-01],\n",
      "         [ 2.0814e-01,  9.0846e-01, -3.6274e-01,  3.2300e-01],\n",
      "         [-4.4585e-01,  4.1799e-01,  7.9162e-01,  1.9580e-01]],\n",
      "\n",
      "        [[ 9.9572e-01,  8.8480e-03, -9.1977e-02,  2.7466e-02],\n",
      "         [-1.1215e-02,  9.9962e-01, -2.5239e-02,  3.0347e-01],\n",
      "         [ 9.1732e-02,  2.6155e-02,  9.9544e-01, -1.8115e-01]],\n",
      "\n",
      "        [[-7.8656e-01,  5.1188e-02, -6.1474e-01,  5.4834e-01],\n",
      "         [-3.8947e-01,  7.3197e-01,  5.5903e-01, -4.5825e-01],\n",
      "         [ 4.7865e-01,  6.7924e-01, -5.5591e-01,  1.1475e+00]],\n",
      "\n",
      "        [[ 4.8963e-02, -1.2145e-01,  9.9111e-01, -8.7354e-01],\n",
      "         [ 5.9721e-01,  7.9889e-01,  6.8176e-02, -8.1177e-02],\n",
      "         [-8.0051e-01,  5.8939e-01,  1.1152e-01,  6.6846e-01]]],\n",
      "       device='cuda:0')\n",
      "width: 1236 | w: 518 | scale: 2.386100386100386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 3, 350, 518])\n",
      "(39, 350, 518, 3)\n",
      "Saved point cloud with 1000000 points to /home/skhalid/Documents/data/360_v2/bicycle/sparse/0/points3D.ply\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run inference here\n",
    "'''\n",
    "\n",
    "BATCH_SIZE=7\n",
    "SINGLE_SAMPLE = False\n",
    "normals = None\n",
    "\n",
    "predictions = run_batched_camera_inference(model, image_names, batch_size=BATCH_SIZE)\n",
    "\n",
    "all_extrinsics = predictions[\"all_extrinsics\"]\n",
    "all_intrinsics = predictions[\"all_intrinsics\"]\n",
    "all_world_points = predictions[\"all_world_points\"]\n",
    "depth_conf_maps = predictions[\"depth_conf_maps\"]\n",
    "\n",
    "b, h, w, c = all_world_points.shape\n",
    "images = convert_to_colmap_format(image_names, all_extrinsics, image_name_prefix=PREFIX, start_id=START_ID)\n",
    "write_extrinsics_binary(images, EXTRINSICS_BINARY_PATH)\n",
    "write_intrinsics_binary_from_poses(all_intrinsics, width, height, (w, h), INTRINSICS_BINARY_PATH)\n",
    "\n",
    "if SINGLE_SAMPLE:\n",
    "    all_images = load_and_preprocess_images(image_names).to(device)\n",
    "    all_images = all_images[0][None, ...]\n",
    "    # Remove batch dimension\n",
    "    world_points = all_world_points[0]  # shape: [16, 350, 518, 3]\n",
    "else:\n",
    "    all_images = load_and_preprocess_images(image_names).to(device)\n",
    "    # Remove batch dimension\n",
    "    world_points = all_world_points  # shape: [16, 350, 518, 3]\n",
    "\n",
    "print(all_images.shape)\n",
    "print(all_world_points.shape)\n",
    "\n",
    "# Flatten to [N, 3]\n",
    "points_pmap = world_points.reshape(-1, 3)  # shape: [16 * 350 * 518, 3]\n",
    "colors_pmap = all_images.permute(0,2,3,1).reshape(-1, 3).cpu().numpy()\n",
    "\n",
    "# Convert to NumPy\n",
    "points_np = points_pmap\n",
    "colors_np = colors_pmap\n",
    "\n",
    "# Optional: Remove invalid points (e.g., zero points)\n",
    "mask = np.logical_and.reduce([np.isfinite(points_np[:, 0]),\n",
    "                              np.isfinite(points_np[:, 1]),\n",
    "                              np.isfinite(points_np[:, 2])])\n",
    "points_np = points_np[mask]\n",
    "colors_np = colors_np[mask]\n",
    "\n",
    "if len(points_np) >= N:\n",
    "    indices = np.random.choice(len(points_np), N, replace=False)\n",
    "    sampled_points = points_np[indices]\n",
    "    sampled_colors = colors_np[indices]\n",
    "else:\n",
    "    print(f\"Warning: Only {len(points_np)} points available, returning all.\")\n",
    "    sampled_points = points_np\n",
    "    sampled_colors = colors_np\n",
    "\n",
    "# Save as PLY\n",
    "save_point_cloud_ply(PTS_PATH, sampled_points, sampled_colors, normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8358f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSAMPLE OBJECT SEGMENTATION (BACKUP)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SAMPLE OBJECT SEGMENTATION (BACKUP)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bf63765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model = YOLO(\"yolo11n-seg\")\n",
    "\n",
    "# # results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
    "# # results = model([\"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"])\n",
    "# # results = model([\"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"])\n",
    "# results = model([\"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"])\n",
    "\n",
    "# for result in results:\n",
    "#     result.show()\n",
    "\n",
    "# # fig = plt.figure(figsize=(10,10))\n",
    "# # plt.imshow(results.render()[0])\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf205ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "# from PIL import Image\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# import clip\n",
    "# import urllib.request\n",
    "# from torchvision import transforms\n",
    "\n",
    "# # Load image\n",
    "# # image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image_rgb = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "# # Step 1: Load Segment Anything Model\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# model_type = \"vit_l\"\n",
    "\n",
    "# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "# sam.eval().cuda()\n",
    "\n",
    "# predictor = SamPredictor(sam)\n",
    "# predictor.set_image(image_rgb)\n",
    "\n",
    "# # Step 2: Predict masks with SAM (automatic mode)\n",
    "# from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "# mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "# masks = mask_generator.generate(image_rgb)\n",
    "\n",
    "# # Step 3: Load CLIP\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# # Step 4: Classify each mask region with CLIP\n",
    "# def classify_mask_with_clip(image_rgb, mask, clip_model, preprocess):\n",
    "#     # Crop the mask region\n",
    "#     # x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8)).tolist()\n",
    "#     x0, y0, x1, y1 = cv2.boundingRect(mask.astype(np.uint8))\n",
    "#     cropped = image_rgb[y0:y1, x0:x1]\n",
    "\n",
    "#     # Apply mask\n",
    "#     masked_image = image_rgb.copy()\n",
    "#     masked_image[~mask.astype(bool)] = 0  # Black out background\n",
    "    \n",
    "#     if cropped.shape[0] < 10 or cropped.shape[1] < 10:\n",
    "#         return \"Unknown\", 0.0\n",
    "\n",
    "#     pil_crop = Image.fromarray(cropped)\n",
    "#     image_input = preprocess(pil_crop).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Define your candidate labels\n",
    "#     labels = [\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]\n",
    "#     # text_inputs = torch.cat([clip.tokenize(f\"a cropped snippet of an image extracted from the tanks and temples dataset that looks like {c}\") for c in labels]).to(device)\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"the cropped image extracted from the mipnerf360 dataset that looks like {c}\") for c in labels]).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         image_features = clip_model.encode_image(image_input)\n",
    "#         text_features = clip_model.encode_text(text_inputs)\n",
    "\n",
    "#         image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#         text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#         label_id = similarity.argmax().item()\n",
    "\n",
    "#         confidence = similarity[0, label_id].item()\n",
    "#         if confidence < 0.3:\n",
    "#             return \"Unknown\", confidence\n",
    "\n",
    "#         return labels[label_id], confidence\n",
    "\n",
    "# # Step 5: Visualize masks and CLIP labels\n",
    "# output_img = image_rgb.copy()\n",
    "# for mask_data in masks:\n",
    "#     mask = mask_data['segmentation']\n",
    "#     label, confidence = classify_mask_with_clip(image_rgb, mask, clip_model, preprocess)\n",
    "\n",
    "#     if label == \"Unknown\":\n",
    "#         continue\n",
    "# # \n",
    "#     # Draw mask\n",
    "#     color = np.random.randint(0, 255, size=3)\n",
    "#     output_img[mask] = 0.6 * output_img[mask] + 0.4 * color\n",
    "# # \n",
    "#     # Draw label\n",
    "#     y, x = np.argwhere(mask).mean(axis=0).astype(int)\n",
    "#     cv2.putText(output_img, f\"{label} {confidence:.2f}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 2)\n",
    "\n",
    "# # Show results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(output_img)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Segment Anything + CLIP classification\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b67fc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load OWL-ViT\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\").to(device)\n",
    "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "\n",
    "# # Load SAM\n",
    "# sam_checkpoint = \"/home/skhalid/Downloads/sam_vit_l.pth\"\n",
    "# sam = sam_model_registry[\"vit_l\"](checkpoint=sam_checkpoint).to(device).eval()\n",
    "# predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57857658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load image\n",
    "# image_path = \"/home/skhalid/Documents/data/tandt_db/tandt/truck/images/000001.jpg\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/nerf_synthetic/lego/images/r_0.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/bicycle/images_4/_DSC8679.JPG\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/Synthetic4Relight/hotdog/train/000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/data_dtu/DTU_scan24/inputs/images/000000.png\"\n",
    "# # image_path = \"/home/skhalid/Documents/data/banana/images/frame_00002.JPG\"\n",
    "\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# image_np = np.array(image)\n",
    "\n",
    "# # Text prompt\n",
    "# # texts = [[\"car\", \"bicycle\", \"trees\", \"grass\", \"ground\", \"bench\", \"lego\", \"fruit\"]]  # You can modify this list\n",
    "# texts = [[\"truck\", \"house\", \"ground\", \"tree\", \"chair\", \"building\", \"sky\", \"clouds\", \"road\", \"lego\", \"grass\", \"toy\", \"hotdog\", \"fruit\", \"food\", \"window\"]]\n",
    "\n",
    "# # Prepare input for OWL-ViT\n",
    "# inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Detect with OWL-ViT\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Get boxes and scores\n",
    "# target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "# results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.3)[0]\n",
    "\n",
    "# # Run SAM\n",
    "# predictor.set_image(image_np)\n",
    "\n",
    "# # Process each box from OWL-ViT\n",
    "# for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
    "#     box = box.cpu().numpy().astype(int)\n",
    "#     x0, y0, x1, y1 = box\n",
    "#     print(f\"{texts[0][label]}: {score:.2f} at box {box}\")\n",
    "\n",
    "#     # SAM expects box in XYXY format\n",
    "#     input_box = np.array([x0, y0, x1, y1])\n",
    "#     masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)\n",
    "\n",
    "#     # Overlay mask\n",
    "#     mask = masks[0]\n",
    "#     overlay = image_np.copy()\n",
    "#     overlay[mask] = (255, 0, 0)  # Red mask\n",
    "\n",
    "#     # Draw bounding box\n",
    "#     cv2.rectangle(overlay, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "#     cv2.putText(overlay, f\"{texts[0][label]} {score:.2f}\", (x0, y0 - 10),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # Show\n",
    "#     plt.imshow(overlay)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
